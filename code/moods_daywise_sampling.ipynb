{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cerebralcortex.util.helper_methods import get_study_names\n",
    "sn = get_study_names(\"/home/jupyter/cc3_conf/\")\n",
    "print(sn)\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import StructField, StructType, DoubleType,MapType, StringType,ArrayType, FloatType, TimestampType, IntegerType\n",
    "from pyspark.sql.functions import minute, second, mean, window\n",
    "from pyspark.sql import functions as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from cerebralcortex.core.datatypes import DataStream\n",
    "from cerebralcortex.core.metadata_manager.stream.metadata import Metadata, DataDescriptor, \\\n",
    "ModuleMetadata\n",
    "from typing import List\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "import pandas as pd\n",
    "from cerebralcortex import Kernel\n",
    "from pyspark.sql import functions as F\n",
    "CC = Kernel(\"/home/jupyter/cc3_moods_conf/\", study_name='moods')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppg_stream_name = 'ppg--org.md2k.watch--fossil_watch_sport'\n",
    "acl_stream_name = 'accelerometer--org.md2k.watch--fossil_watch_sport'\n",
    "\n",
    "ppg_data = CC.get_stream(ppg_stream_name)\n",
    "\n",
    "acl_data = CC.get_stream(acl_stream_name)\n",
    "\n",
    "ppg_data  = ppg_data.withColumn('day',F.date_format('localtime',\"yyyyMMdd\"))\n",
    "acl_data = acl_data.withColumn('day',F.date_format('localtime',\"yyyyMMdd\"))\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "minimum_time = datetime(2020,11,25)\n",
    "\n",
    "acl_data = acl_data.filter(F.col('localtime')>minimum_time)\n",
    "ppg_data = ppg_data.filter(F.col('localtime')>minimum_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = acl_data\n",
    "\n",
    "win = F.window(\"timestamp\", windowDuration='5 seconds',slideDuration='2 seconds',startTime='0 seconds')\n",
    "groupbycols = [\"user\",\"version\"] + [win]\n",
    "\n",
    "import numpy as np\n",
    "schema2 = StructType([\n",
    "    StructField(\"version\", IntegerType()),\n",
    "    StructField(\"user\", StringType()),\n",
    "    StructField(\"start\", TimestampType()),\n",
    "    StructField(\"end\", TimestampType()),\n",
    "    StructField(\"timestamp\", TimestampType()),\n",
    "    StructField(\"localtime\", TimestampType()),\n",
    "    StructField(\"activity\", DoubleType())\n",
    "])\n",
    "@pandas_udf(schema2, PandasUDFType.GROUPED_MAP)\n",
    "def compute_std(key,data1):\n",
    "    activity = np.sqrt(data1['x'].std()**2 + data1['y'].std()**2 + data1['z'].std()**2)\n",
    "    temp = [data1.version.values[0],\n",
    "           data1.user.values[0],\n",
    "           key[2]['start'],\n",
    "           key[2]['end'],\n",
    "           data1['timestamp'].values[0],\n",
    "           data1['localtime'].values[0],\n",
    "           activity]\n",
    "    return pd.DataFrame([temp],columns=['version','user','start','end','timestamp','localtime','activity'])\n",
    "activity_data = data._data.groupBy(groupbycols).apply(compute_std)\n",
    "schema = activity_data.schema\n",
    "stream_metadata = Metadata()\n",
    "stream_metadata.set_name(\"org.md2k.acl.std.5.secs\").set_description(\"acl std 5 secs\")\n",
    "for field in schema.fields:\n",
    "    stream_metadata.add_dataDescriptor(\n",
    "        DataDescriptor().set_name(str(field.name)).set_type(str(field.dataType))\n",
    "    )\n",
    "stream_metadata.add_module(\n",
    "    ModuleMetadata().set_name(\"acl std 5 secs\") \\\n",
    "    .set_attribute(\"url\", \"https://md2k.org\").set_author(\n",
    "        \"Md Azim Ullah\", \"mullah@memphis.edu\"))\n",
    "stream_metadata.is_valid()\n",
    "ds = DataStream(data=activity_data,metadata=stream_metadata)\n",
    "CC.save_stream(ds,overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CC.get_stream(\"org.md2k.acl.std.5.secs\")\n",
    "\n",
    "win = F.window(\"timestamp\", windowDuration='60 seconds',startTime='0 seconds')\n",
    "groupbycols = [\"user\",\"version\"] + [win]\n",
    "\n",
    "import numpy as np\n",
    "schema2 = StructType([\n",
    "    StructField(\"version\", IntegerType()),\n",
    "    StructField(\"user\", StringType()),\n",
    "    StructField(\"start\", TimestampType()),\n",
    "    StructField(\"end\", TimestampType()),\n",
    "    StructField(\"timestamp\", TimestampType()),\n",
    "    StructField(\"localtime\", TimestampType()),\n",
    "    StructField(\"std\", DoubleType())\n",
    "])\n",
    "@pandas_udf(schema2, PandasUDFType.GROUPED_MAP)\n",
    "def compute_std(key,data1):\n",
    "    activity = np.percentile(data1['activity'],25)\n",
    "    temp = [data1.version.values[0],\n",
    "           data1.user.values[0],\n",
    "           key[2]['start'],\n",
    "           key[2]['end'],\n",
    "           data1['timestamp'].values[0],\n",
    "           data1['localtime'].values[0],\n",
    "           activity]\n",
    "    return pd.DataFrame([temp],columns=['version','user','start','end','timestamp','localtime','std'])\n",
    "activity_data = data._data.groupBy(groupbycols).apply(compute_std)\n",
    "schema = activity_data.schema\n",
    "stream_metadata = Metadata()\n",
    "stream_metadata.set_name(\"org.md2k.acl.std.60.secs.25.percentile\").set_description(\"acl std 60 secs 25 percentile\")\n",
    "for field in schema.fields:\n",
    "    stream_metadata.add_dataDescriptor(\n",
    "        DataDescriptor().set_name(str(field.name)).set_type(str(field.dataType))\n",
    "    )\n",
    "stream_metadata.add_module(\n",
    "    ModuleMetadata().set_name(\"acl std 60 secs 25 percentile\") \\\n",
    "    .set_attribute(\"url\", \"https://md2k.org\").set_author(\n",
    "        \"Md Azim Ullah\", \"mullah@memphis.edu\"))\n",
    "stream_metadata.is_valid()\n",
    "ds = DataStream(data=activity_data,metadata=stream_metadata)\n",
    "CC.save_stream(ds,overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import StructField, StructType, DoubleType, StringType, TimestampType, IntegerType\n",
    "import numpy as np\n",
    "from cerebralcortex.core.datatypes import DataStream\n",
    "from cerebralcortex.core.metadata_manager.stream.metadata import Metadata, DataDescriptor, \\\n",
    "    ModuleMetadata\n",
    "from scipy import signal\n",
    "def filter_data(X,\n",
    "                Fs=100,\n",
    "                low_cutoff=.4,\n",
    "                high_cutoff=3.0,\n",
    "                filter_order=65):\n",
    "    \"\"\"\n",
    "    Bandpass Filter of single channel\n",
    "\n",
    "    :param X: input data\n",
    "    :param Fs: sampling freq.\n",
    "    :param low_cutoff: low passband\n",
    "    :param high_cutoff: high passband\n",
    "    :param filter_order: no of taps in FIR filter\n",
    "\n",
    "    :return: filtered version of input data\n",
    "    \"\"\"\n",
    "    X1 = X.reshape(-1,1)\n",
    "    X1 = signal.detrend(X1,axis=0,type='constant')\n",
    "    b = signal.firls(filter_order,np.array([0,low_cutoff-.1, low_cutoff, high_cutoff ,high_cutoff+.5,Fs/2]),np.array([0, 0 ,1 ,1 ,0, 0]),\n",
    "                     np.array([100*0.02,0.02,0.02]),fs=Fs)\n",
    "    X2 = signal.convolve(X1.reshape(-1),b,mode='same')\n",
    "    return X2\n",
    "\n",
    "def get_metadata(data,\n",
    "                 wrist='left',\n",
    "                 sensor_name='motionsensehrv',\n",
    "                 ppg_columns=('red','infrared','green'),\n",
    "                 acl_columns=('aclx','acly','aclz')):\n",
    "    \"\"\"\n",
    "    :param data: input stream\n",
    "    :param wrist: which wrist the data was collected from\n",
    "    :param sensor_name: name of sensor\n",
    "    :param ppg_columns: columns in the input dataframe referring to multiple ppg channels\n",
    "    :param acl_columns: columns in the input dataframe referring to accelerometer channels\n",
    "\n",
    "    :return: metadata of output stream\n",
    "    \"\"\"\n",
    "    stream_name = \"org.md2k.\"+str(sensor_name)+\".\"+str(wrist)+\".wrist.bandpass.filtered\"\n",
    "    stream_metadata = Metadata()\n",
    "    stream_metadata.set_name(stream_name).set_description(\"Bandpass Filtered PPG data\") \\\n",
    "        .add_dataDescriptor(DataDescriptor().set_name(\"timestamp\").set_type(\"datetime\")) \\\n",
    "        .add_dataDescriptor(DataDescriptor().set_name(\"localtime\").set_type(\"datetime\")) \\\n",
    "        .add_dataDescriptor(DataDescriptor().set_name(\"version\").set_type(\"int\")) \\\n",
    "        .add_dataDescriptor(DataDescriptor().set_name(\"user\").set_type(\"string\"))\n",
    "\n",
    "    for c in ppg_columns:\n",
    "        stream_metadata.add_dataDescriptor(DataDescriptor().set_name(c).set_type(\"double\").set_attribute(\"description\",\n",
    "                                                                                                    \"ppg channel \"+c))\n",
    "    for c in acl_columns:\n",
    "        stream_metadata.add_dataDescriptor(DataDescriptor().set_name(c).set_type(\"double\").set_attribute(\"description\",\n",
    "                                                                                            \"accelerometer channel \"+c))\n",
    "\n",
    "    stream_metadata.add_module(\n",
    "        ModuleMetadata().set_name(\"ecg data quality\").set_attribute(\"url\", \"http://md2k.org/\").set_author(\n",
    "            \"Md Azim Ullah\", \"mullah@memphis.edu\"))\n",
    "    return stream_metadata\n",
    "\n",
    "\n",
    "def bandpass_filter(\n",
    "                   data,\n",
    "                   Fs = 25,\n",
    "                   low_cutoff = 0.4,\n",
    "                   high_cutoff = 3.0,\n",
    "                   filter_order = 65,\n",
    "                   ppg_columns=('red','infrared','green'),\n",
    "                   acl_columns=('aclx','acly','aclz'),\n",
    "                   wrist='left',\n",
    "                   sensor_name='motionsensehrv'):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    :param data: PPG & ACL data stream\n",
    "    :param Fs: sampling frequency\n",
    "    :param low_cutoff: minimum frequency of pass band\n",
    "    :param high_cutoff: Maximum Frequency of pass band\n",
    "    :param filter_order: no. of taps in FIR filter\n",
    "    :param ppg_columns: columns in the input dataframe referring to multiple ppg channels\n",
    "    :param acl_columns: columns in the input dataframe referring to accelerometer channels\n",
    "    :param wrist: which wrist the data was collected from\n",
    "    :param sensor_name: name of sensor\n",
    "\n",
    "    :return: Bandpass filtered version of input PPG data\n",
    "    \"\"\"\n",
    "\n",
    "    ## check if all columns exist\n",
    "\n",
    "    default_columns = ['user','version','localtime','timestamp']\n",
    "    required_columns = default_columns+acl_columns+ppg_columns\n",
    "    if len(set(required_columns)-set(data.columns))>0:\n",
    "        raise Exception(\"Columns missing in input dataframe! \" + str(list(set(required_columns)-set(data.columns))))\n",
    "\n",
    "    ## select the columns from input dataframe\n",
    "\n",
    "    data = data.select(*required_columns)\n",
    "\n",
    "    ## udf\n",
    "\n",
    "    default_schema = [StructField(\"timestamp\", TimestampType()),\n",
    "                      StructField(\"localtime\", TimestampType()),\n",
    "                      StructField(\"version\", IntegerType()),\n",
    "                      StructField(\"user\", StringType())]\n",
    "    schema = StructType(default_schema+[StructField(c, DoubleType()) for c in list(ppg_columns)+list(acl_columns)])\n",
    "    @pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "    def ppg_bandpass(data):\n",
    "        data = data.sort_values('timestamp').reset_index(drop=True)\n",
    "        for c in ppg_columns:\n",
    "            data[c] = filter_data(data[c].values,Fs=Fs,low_cutoff=low_cutoff,high_cutoff=high_cutoff,filter_order=filter_order)\n",
    "        return data\n",
    "\n",
    "    ## steps\n",
    "    ppg_bandpass_filtered = data.compute(ppg_bandpass,windowDuration=60*60*10,startTime='0 seconds')\n",
    "    output_data = ppg_bandpass_filtered._data\n",
    "    ds = DataStream(data=output_data,metadata=get_metadata(data,wrist=wrist,sensor_name=sensor_name,\n",
    "                                                           ppg_columns=ppg_columns,acl_columns=acl_columns))\n",
    "    return ds\n",
    "\n",
    "data = CC.get_stream('ppg--org.md2k.watch--fossil_watch_sport')\n",
    "data  = data.withColumn('day',F.date_format('localtime',\"yyyyMMdd\"))\n",
    "filtered_data = bandpass_filter(\n",
    "                   data,\n",
    "                   Fs = 100,\n",
    "                   low_cutoff = 0.4,\n",
    "                   high_cutoff = 3.0,\n",
    "                   filter_order = 65,\n",
    "                   ppg_columns=['ppg1'],\n",
    "                   acl_columns=[],\n",
    "                   wrist='left',\n",
    "                   sensor_name='fossil')\n",
    "CC.save_stream(filtered_data,overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import iqr\n",
    "data = CC.get_stream('org.md2k.fossil.left.wrist.bandpass.filtered')\n",
    "# data.printSchema()\n",
    "win = F.window(\"timestamp\", windowDuration='60 seconds',startTime='0 seconds')\n",
    "groupbycols = [\"user\",\"version\"] + [win]\n",
    "\n",
    "import numpy as np\n",
    "schema2 = StructType([\n",
    "    StructField(\"version\", IntegerType()),\n",
    "    StructField(\"user\", StringType()),\n",
    "    StructField(\"start\", TimestampType()),\n",
    "    StructField(\"end\", TimestampType()),\n",
    "    StructField(\"timestamp\", TimestampType()),\n",
    "    StructField(\"localtime\", TimestampType()),\n",
    "    StructField(\"iqr\", DoubleType())\n",
    "])\n",
    "@pandas_udf(schema2, PandasUDFType.GROUPED_MAP)\n",
    "def compute_std(key,data1):\n",
    "    if data1.shape[0]<20*100:\n",
    "        activity = 0\n",
    "    else:\n",
    "        activity = iqr(data1['ppg1'])\n",
    "    temp = [data1.version.values[0],\n",
    "           data1.user.values[0],\n",
    "           key[2]['start'],\n",
    "           key[2]['end'],\n",
    "           data1['timestamp'].values[0],\n",
    "           data1['localtime'].values[0],\n",
    "           activity]\n",
    "    return pd.DataFrame([temp],columns=['version','user','start','end','timestamp','localtime','iqr'])\n",
    "activity_data = data._data.groupBy(groupbycols).apply(compute_std)\n",
    "schema = activity_data.schema\n",
    "stream_metadata = Metadata()\n",
    "stream_metadata.set_name(\"org.md2k.ppg.filtered.iqr.60.secs\").set_description(\"filtered ppg inter quartile range\")\n",
    "for field in schema.fields:\n",
    "    stream_metadata.add_dataDescriptor(\n",
    "        DataDescriptor().set_name(str(field.name)).set_type(str(field.dataType))\n",
    "    )\n",
    "stream_metadata.add_module(\n",
    "    ModuleMetadata().set_name(\"filtered ppg inter quartile range\") \\\n",
    "    .set_attribute(\"url\", \"https://md2k.org\").set_author(\n",
    "        \"Md Azim Ullah\", \"mullah@memphis.edu\"))\n",
    "stream_metadata.is_valid()\n",
    "ds = DataStream(data=activity_data,metadata=stream_metadata)\n",
    "CC.save_stream(ds,overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppg_iqr_data = CC.get_stream('org.md2k.ppg.filtered.iqr.60.secs')\n",
    "from datetime import datetime\n",
    "\n",
    "minimum_time = datetime(2020,11,25)\n",
    "ppg_iqr_data = ppg_iqr_data.filter(F.col('localtime')>minimum_time).drop('localtime','timestamp')\n",
    "\n",
    "acl_std_data = CC.get_stream('org.md2k.acl.std.60.secs.25.percentile')\n",
    "\n",
    "all_data = acl_std_data.join(ppg_iqr_data,on=['user','start','end'],how='left')\n",
    "all_data = all_data.withColumn('day',F.date_format('localtime',\"yyyyMMdd\"))\n",
    "all_data = all_data.withColumn('time',F.col('timestamp').cast('double'))\n",
    "\n",
    "all_data = all_data.toPandas()\n",
    "\n",
    "all_data.fillna(0,inplace=True)\n",
    "\n",
    "all_data = all_data.sort_values('timestamp').reset_index(drop=True)\n",
    "import pickle\n",
    "pickle.dump(all_data,open('../data/all_data.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "all_data = pickle.load(open('../data/all_data.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sample:\n",
    "    def __init__(self,acl_std=None,ppg_iqr=None,cur_time=None):\n",
    "        self.last_sampled = 0\n",
    "        self.acl_std = list(acl_std)\n",
    "        self.ppg_iqr = [a>500 for a in ppg_iqr]\n",
    "        self.cur_time = cur_time\n",
    "        \n",
    "    def get_decesion(self,iqr,std,cur_time):\n",
    "        self.last_sampled = cur_time-self.cur_time\n",
    "        self.cur_time = cur_time\n",
    "        self.acl_std.append(std)\n",
    "        if self.last_sampled>10*60:\n",
    "            self.ppg_iqr.append(iqr>500)\n",
    "            return 1\n",
    "        if self.last_sampled>5*60:\n",
    "            if self.ppg_iqr[-1]:\n",
    "                self.ppg_iqr.append(iqr>500)\n",
    "                return 1\n",
    "            return -1\n",
    "        temp_std = np.array(self.acl_std[-3:])\n",
    "        if len(temp_std[temp_std>.0234]):\n",
    "            self.ppg_iqr.append(iqr>500)\n",
    "            return 1\n",
    "        else:\n",
    "#             self.ppg_iqr.append(False)\n",
    "            return -1\n",
    "            \n",
    "        \n",
    "        \n",
    "\n",
    "def get_sampling_decision(df):\n",
    "    df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "#     df['sample'] = -1\n",
    "#     df.iloc[0,'sample'] = 1\n",
    "#     df.iloc[1,'sample'] = 1\n",
    "#     df.iloc[2,'sample'] = 1\n",
    "    sample_col = [1,1,1]\n",
    "    sample_class = Sample(acl_std=df['std'].loc[0:1].values,ppg_iqr=df['iqr'].loc[0:2].values,cur_time=df['time'].loc[1])\n",
    "    for i,row in df.iterrows():\n",
    "        if i<3:\n",
    "            continue\n",
    "        sample = sample_class.get_decesion(df['iqr'].loc[i],df['std'].loc[i-1],df['time'].loc[i-1])\n",
    "#         df.iloc[i,'sample'] = sample\n",
    "        sample_col.append(sample)\n",
    "    df['sample'] = sample_col\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "temp_data1 = all_data.groupby(['user','day'],as_index=False).apply(get_sampling_decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_data = CC.get_stream('moods_logs')\n",
    "\n",
    "log_data = log_data.withColumn('day',F.date_format('timestamp',\"yyyyMMdd\")).toPandas()\n",
    "\n",
    "log_data['ind'] = log_data['message'].apply(lambda a:a.find('ACTIVITY INPUT'))\n",
    "log_data = log_data[log_data.ind>-1]\n",
    "\n",
    "log_data['message'] = log_data['message'].apply(lambda a:a[17:-1])\n",
    "\n",
    "# log_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "plt.rcParams.update({'font.size':25})\n",
    "import matplotlib.pyplot as plt\n",
    "for user in np.unique(temp_data1['user'].values):\n",
    "    for day in np.unique(temp_data1['day'].values):\n",
    "        temp_data = temp_data1[(temp_data1.day==day)&(temp_data1.user==user)].sort_values('timestamp').reset_index(drop=True)\n",
    "        \n",
    "#         temp_log = log_data[(log_data.uuid==user)&(log_data.day==day)]\n",
    "        if temp_data.shape[0]<5:\n",
    "            continue\n",
    "        iqr_list = np.array([0]*temp_data.shape[0])\n",
    "        iqr_list[temp_data['iqr']<500] = 0\n",
    "        iqr_list[temp_data['iqr']>=500] = 1\n",
    "        fig,ax = plt.subplots(3,1,figsize=(20,15),sharex=True)\n",
    "        ax[0].plot(temp_data['timestamp'],temp_data['sample'],'*--')\n",
    "        ax[0].set_title('Sampling Decision')\n",
    "    #     plt.show()\n",
    "    #     plt.figure(figsize=(10,10))\n",
    "        ax[1].bar(temp_data['timestamp'],temp_data['std'],.001,color='red')\n",
    "        ax[1].set_title('ACL STD')\n",
    "        ax[2].bar(temp_data['timestamp'],iqr_list,.001,color='green')\n",
    "        ax[2].set_title('Status of PPG Data')\n",
    "#         plt.setp(ax[1],xticks= temp_log['timestamp'],xticklabels=temp_log['message'])\n",
    "        plt.xticks(rotation=60)\n",
    "#         plt.setp(ax[2],xticks= temp_log['timestamp'],xticklabels=temp_log['timestamp'])\n",
    "#         for i,row in temp_log.iterrows():\n",
    "#             ax[3].text(row['timestamp'], 0.5, row['message'],fontsize=20)\n",
    "        plt.savefig('../images_sampling/'+str(count)+'.pdf',dps=1e6)\n",
    "        count+=1\n",
    "        plt.show()\n",
    "        print('-'*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_data[log_data.day=='20201214']\n",
    "import shutil\n",
    "shutil.make_archive('../images_sampling/','zip','../images_sampling/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = schema2 = StructType([\n",
    "    StructField(\"version\", IntegerType()),\n",
    "    StructField(\"user\", StringType()),\n",
    "    StructField(\"std\", DoubleType()),\n",
    "    StructField(\"day\", StringType()),\n",
    "    StructField(\"timestamp\", TimestampType()),\n",
    "    StructField(\"localtime\", TimestampType())    \n",
    "])\n",
    "\n",
    "\n",
    "def get_std(df_5):\n",
    "    return np.sqrt(df_5['x'].std()**2+df_5['y'].std()**2+df_5['z'].std()**2)\n",
    "    \n",
    "\n",
    "def minute_level_processing(df_min):\n",
    "    std_5_secs = [get_std(df_5) for i,df_5 in df_min.groupby(pd.Grouper(key='timestamp',freq='5S'))]\n",
    "    return np.percentile(std_5_secs,25)\n",
    "\n",
    "@pandas_udf(schema,PandasUDFType.GROUPED_MAP)\n",
    "def temp(df):\n",
    "    if df.shape[0]<100:\n",
    "        return pd.DataFrame([],columns=['version','user','std','day','timestamp','localtime'])\n",
    "    df_minutewise = pd.DataFrame([[1,df_min['user'].values[0],minute_level_processing(df_min),df_min['day'].values[0],df_min['timestamp'].values[0],df_min['localtime'].values[0]]\n",
    "                       for i,df_min in df.groupby(pd.Grouper(key='timestamp',freq='60S')) if df_min.shape[0]>100],columns=['version','user','std','day','timestamp','localtime'])\n",
    "    return df_minutewise\n",
    "acl_data_sampling = acl_data._data.groupBy(['day','user','version']).apply(temp)\n",
    "\n",
    "schema = acl_data_sampling.schema\n",
    "stream_metadata = Metadata()\n",
    "stream_metadata.set_name(\"org.md2k.acl.std.25.percentile.minute\").set_description(\"acl std 25th percentile\")\n",
    "for field in schema.fields:\n",
    "    stream_metadata.add_dataDescriptor(\n",
    "        DataDescriptor().set_name(str(field.name)).set_type(str(field.dataType))\n",
    "    )\n",
    "stream_metadata.add_module(\n",
    "    ModuleMetadata().set_name(\"ACL std 25th percentile\") \\\n",
    "    .set_attribute(\"url\", \"https://md2k.org\").set_author(\n",
    "        \"Md Azim Ullah\", \"mullah@memphis.edu\"))\n",
    "stream_metadata.is_valid()\n",
    "ds = DataStream(data=acl_data_sampling,metadata=stream_metadata)\n",
    "CC.save_stream(ds,overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_data = CC.get_stream(\"org.md2k.acl.std.25.percentile.minute\")\n",
    "std_data = std_data.withColumn('time',F.col('timestamp').cast('double'))\n",
    "std_data = std_data.withColumn('std_final',F.array('time','std')).drop(*['std'])\n",
    "win = F.window(\"timestamp\", windowDuration='60 seconds', startTime='0 seconds')\n",
    "groupbycols = [\"user\"] + [win]\n",
    "std_data = std_data.groupby(groupbycols).agg(F.collect_list('std_final'),F.collect_list('localtime')).withColumnRenamed('collect_list(std_final)','std_final').withColumnRenamed('collect_list(localtime)','localtime')\n",
    "std_data = std_data.withColumn('localtime',F.col('localtime').getItem(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_data.show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CC3.3",
   "language": "python",
   "name": "cc33"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
