{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cerebralcortex.util.helper_methods import get_study_names\n",
    "sn = get_study_names(\"/home/jupyter/cc3_conf/\")\n",
    "print(sn)\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import StructField, StructType, DoubleType,MapType, StringType,ArrayType, FloatType, TimestampType, IntegerType\n",
    "from pyspark.sql.functions import minute, second, mean, window\n",
    "from pyspark.sql import functions as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from cerebralcortex.core.datatypes import DataStream\n",
    "from cerebralcortex.core.metadata_manager.stream.metadata import Metadata, DataDescriptor, \\\n",
    "ModuleMetadata\n",
    "from typing import List\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "import pandas as pd\n",
    "from cerebralcortex import Kernel\n",
    "from pyspark.sql import functions as F\n",
    "CC = Kernel(\"/home/jupyter/cc3_moods_conf/\", study_name='moods')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bandpass filter the PPG data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import StructField, StructType, DoubleType, StringType, TimestampType, IntegerType\n",
    "import numpy as np\n",
    "from cerebralcortex.core.datatypes import DataStream\n",
    "from cerebralcortex.core.metadata_manager.stream.metadata import Metadata, DataDescriptor, \\\n",
    "    ModuleMetadata\n",
    "from scipy import signal\n",
    "def filter_data(X,\n",
    "                Fs=100,\n",
    "                low_cutoff=.4,\n",
    "                high_cutoff=3.0,\n",
    "                filter_order=65):\n",
    "    \"\"\"\n",
    "    Bandpass Filter of single channel\n",
    "\n",
    "    :param X: input data\n",
    "    :param Fs: sampling freq.\n",
    "    :param low_cutoff: low passband\n",
    "    :param high_cutoff: high passband\n",
    "    :param filter_order: no of taps in FIR filter\n",
    "\n",
    "    :return: filtered version of input data\n",
    "    \"\"\"\n",
    "    X1 = X.reshape(-1,1)\n",
    "    X1 = signal.detrend(X1,axis=0,type='constant')\n",
    "    b = signal.firls(filter_order,np.array([0,low_cutoff-.1, low_cutoff, high_cutoff ,high_cutoff+.5,Fs/2]),np.array([0, 0 ,1 ,1 ,0, 0]),\n",
    "                     np.array([100*0.02,0.02,0.02]),fs=Fs)\n",
    "    X2 = signal.convolve(X1.reshape(-1),b,mode='same')\n",
    "    return X2\n",
    "\n",
    "\n",
    "from scipy.signal import butter, lfilter\n",
    "\n",
    "\n",
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return b, a\n",
    "\n",
    "\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    y = lfilter(b, a, data)\n",
    "    return y\n",
    "\n",
    "def get_metadata(data,\n",
    "                 wrist='left',\n",
    "                 sensor_name='motionsensehrv',\n",
    "                 ppg_columns=('red','infrared','green'),\n",
    "                 acl_columns=('aclx','acly','aclz')):\n",
    "    \"\"\"\n",
    "    :param data: input stream\n",
    "    :param wrist: which wrist the data was collected from\n",
    "    :param sensor_name: name of sensor\n",
    "    :param ppg_columns: columns in the input dataframe referring to multiple ppg channels\n",
    "    :param acl_columns: columns in the input dataframe referring to accelerometer channels\n",
    "\n",
    "    :return: metadata of output stream\n",
    "    \"\"\"\n",
    "    stream_name = \"org.md2k.\"+str(sensor_name)+\".\"+str(wrist)+\".wrist.bandpass.filtered\"\n",
    "    print(stream_name)\n",
    "    stream_metadata = Metadata()\n",
    "    stream_metadata.set_name(stream_name).set_description(\"Bandpass Filtered PPG data\") \\\n",
    "        .add_dataDescriptor(DataDescriptor().set_name(\"timestamp\").set_type(\"datetime\")) \\\n",
    "        .add_dataDescriptor(DataDescriptor().set_name(\"localtime\").set_type(\"datetime\")) \\\n",
    "        .add_dataDescriptor(DataDescriptor().set_name(\"version\").set_type(\"int\")) \\\n",
    "        .add_dataDescriptor(DataDescriptor().set_name(\"user\").set_type(\"string\"))\n",
    "\n",
    "    for c in ppg_columns:\n",
    "        stream_metadata.add_dataDescriptor(DataDescriptor().set_name(c).set_type(\"double\").set_attribute(\"description\",\n",
    "                                                                                                    \"ppg channel \"+c))\n",
    "    for c in acl_columns:\n",
    "        stream_metadata.add_dataDescriptor(DataDescriptor().set_name(c).set_type(\"double\").set_attribute(\"description\",\n",
    "                                                                                            \"accelerometer channel \"+c))\n",
    "\n",
    "    stream_metadata.add_module(\n",
    "        ModuleMetadata().set_name(\"ecg data quality\").set_attribute(\"url\", \"http://md2k.org/\").set_author(\n",
    "            \"Md Azim Ullah\", \"mullah@memphis.edu\"))\n",
    "    return stream_metadata\n",
    "\n",
    "\n",
    "def bandpass_filter(\n",
    "                   data,\n",
    "                   Fs = 25,\n",
    "                   low_cutoff = 0.4,\n",
    "                   high_cutoff = 3.0,\n",
    "                   filter_order = 65,\n",
    "                   ppg_columns=('red','infrared','green'),\n",
    "                   acl_columns=('aclx','acly','aclz'),\n",
    "                   wrist='left',\n",
    "                   sensor_name='motionsensehrv'):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    :param data: PPG & ACL data stream\n",
    "    :param Fs: sampling frequency\n",
    "    :param low_cutoff: minimum frequency of pass band\n",
    "    :param high_cutoff: Maximum Frequency of pass band\n",
    "    :param filter_order: no. of taps in FIR filter\n",
    "    :param ppg_columns: columns in the input dataframe referring to multiple ppg channels\n",
    "    :param acl_columns: columns in the input dataframe referring to accelerometer channels\n",
    "    :param wrist: which wrist the data was collected from\n",
    "    :param sensor_name: name of sensor\n",
    "\n",
    "    :return: Bandpass filtered version of input PPG data\n",
    "    \"\"\"\n",
    "\n",
    "    ## check if all columns exist\n",
    "\n",
    "    default_columns = ['user','version','localtime','timestamp']\n",
    "    required_columns = default_columns+acl_columns+ppg_columns\n",
    "    if len(set(required_columns)-set(data.columns))>0:\n",
    "        raise Exception(\"Columns missing in input dataframe! \" + str(list(set(required_columns)-set(data.columns))))\n",
    "\n",
    "    ## select the columns from input dataframe\n",
    "\n",
    "    data = data.select(*required_columns)\n",
    "\n",
    "    ## udf\n",
    "\n",
    "    default_schema = [StructField(\"timestamp\", TimestampType()),\n",
    "                      StructField(\"localtime\", TimestampType()),\n",
    "                      StructField(\"version\", IntegerType()),\n",
    "                      StructField(\"user\", StringType())]\n",
    "    schema = StructType(default_schema+[StructField(c, DoubleType()) for c in list(ppg_columns)+list(acl_columns)])\n",
    "    @pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "    def ppg_bandpass(data):\n",
    "        data = data.sort_values('timestamp').reset_index(drop=True)\n",
    "        for c in ppg_columns:\n",
    "#             data[c] = filter_data(data[c].values,Fs=Fs,low_cutoff=low_cutoff,high_cutoff=high_cutoff,filter_order=filter_order)\n",
    "            data[c] = butter_bandpass_filter(data[c].values, low_cutoff, high_cutoff, fs=Fs, order=5)\n",
    "        return data\n",
    "\n",
    "    ## steps\n",
    "    ppg_bandpass_filtered = data.compute(ppg_bandpass,windowDuration=60*60*10,startTime='0 seconds')\n",
    "    output_data = ppg_bandpass_filtered._data\n",
    "    ds = DataStream(data=output_data,metadata=get_metadata(data,wrist=wrist,sensor_name=sensor_name,\n",
    "                                                           ppg_columns=ppg_columns,acl_columns=acl_columns))\n",
    "    return ds\n",
    "\n",
    "data = CC.get_stream('ppg--org.md2k.watch--fossil_watch_sport')\n",
    "from datetime import datetime\n",
    "data = data.filter(F.col('localtime')>datetime(2020,12,1))\n",
    "data  = data.withColumn('day',F.date_format('localtime',\"YYYYMMdd\"))\n",
    "filtered_data = bandpass_filter(\n",
    "                   data,\n",
    "                   Fs = 100,\n",
    "                   low_cutoff = 0.4,\n",
    "                   high_cutoff = 3.0,\n",
    "                   filter_order = 65,\n",
    "                   ppg_columns=['ppg1'],\n",
    "                   acl_columns=[],\n",
    "                   wrist='left',\n",
    "                   sensor_name='fossil')\n",
    "CC.save_stream(filtered_data,overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aggregate_second_level_data(data,input_columns,window_size,sliding_size,stream_name,description):\n",
    "    import numpy as np\n",
    "    default_schema = [\n",
    "        StructField(\"version\", IntegerType()),\n",
    "        StructField(\"user\", StringType()),\n",
    "        StructField(\"start\", TimestampType()),\n",
    "        StructField(\"end\", TimestampType()),\n",
    "        StructField(\"timestamp\", TimestampType()),\n",
    "        StructField(\"localtime\", TimestampType())\n",
    "    ]\n",
    "    new_schema = [StructField(a, ArrayType(DoubleType())) for a in input_columns]\n",
    "    schema = StructType(default_schema+new_schema)\n",
    "    @pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "    def compute_window(key,data1):\n",
    "        data1 = data1.sort_values('timestamp').reset_index(drop=True)\n",
    "        temp = [data1.version.values[0],\n",
    "               data1.user.values[0],\n",
    "               key[2]['start'],\n",
    "               key[2]['end'],\n",
    "               data1['timestamp'].values[0],\n",
    "               data1['localtime'].values[0]]\n",
    "        for a in input_columns:\n",
    "            temp.append(data1[a].values)\n",
    "        return pd.DataFrame([temp],columns=['version','user','start','end','timestamp','localtime']+input_columns)\n",
    "    win = F.window(\"timestamp\", windowDuration=str(window_size)+' seconds',startTime='0 seconds',slideDuration=str(sliding_size)+' seconds')\n",
    "    groupbycols = [\"user\",\"version\"] + [win]\n",
    "    activity_data = data._data.groupBy(groupbycols).apply(compute_window)\n",
    "    schema = activity_data.schema\n",
    "    stream_metadata = Metadata()\n",
    "    stream_metadata.set_name(stream_name).set_description(description)\n",
    "    for field in schema.fields:\n",
    "        stream_metadata.add_dataDescriptor(\n",
    "            DataDescriptor().set_name(str(field.name)).set_type(str(field.dataType))\n",
    "        )\n",
    "    stream_metadata.add_module(\n",
    "        ModuleMetadata().set_name(description) \\\n",
    "        .set_attribute(\"url\", \"https://md2k.org\").set_author(\n",
    "            \"Md Azim Ullah\", \"mullah@memphis.edu\"))\n",
    "#     stream_metadata.is_valid()\n",
    "    ds = DataStream(data=activity_data,metadata=stream_metadata)\n",
    "    return ds\n",
    "\n",
    "\n",
    "ppg_data  = CC.get_stream('org.md2k.fossil.left.wrist.bandpass.filtered')\n",
    "ppg_data = ppg_data.withColumn('time',F.col('timestamp').cast('double'))\n",
    "from datetime import datetime\n",
    "ppg_data = ppg_data.filter(F.col('localtime')>datetime(2020,12,1))\n",
    "ppg_5_secs_data = get_aggregate_second_level_data(ppg_data,['ppg1','time'],5,2,\"org.md2k.fossil.ppg.filtered.5secs\",'filtered ppg in 5 seconds')\n",
    "CC.save_stream(ppg_5_secs_data,overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acl_data = CC.get_stream('accelerometer--org.md2k.watch--fossil_watch_sport')\n",
    "acl_data = acl_data.withColumn('time',F.col('timestamp').cast('double'))\n",
    "acl_data = acl_data.filter(F.col('localtime')>datetime(2020,12,1))\n",
    "acl_5_secs_data = get_aggregate_second_level_data(acl_data,['time','x','y','z'],5,2,\"org.md2k.fossil.acl.5secs\",'acl in 5 seconds')\n",
    "CC.save_stream(acl_5_secs_data,overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppg_data = CC.get_stream(\"org.md2k.fossil.ppg.filtered.5secs\").drop('timestamp','localtime','version').withColumnRenamed('time','ppg_time')\n",
    "\n",
    "acl_data = CC.get_stream(\"org.md2k.fossil.acl.5secs\")\n",
    "\n",
    "# ppg_data.printSchema(), acl_data.printSchema()\n",
    "all_data = acl_data.join(ppg_data,on=['start','end','user'],how='inner')\n",
    "\n",
    "all_data = all_data.withColumn('day',F.date_format('localtime',\"yyyyMMdd\"))\n",
    "\n",
    "schema = all_data._data.schema\n",
    "stream_metadata = Metadata()\n",
    "stream_metadata.set_name(\"org.md2k.fossil.ppg.filtered.acl.5secs\").set_description('ppg and acl')\n",
    "for field in schema.fields:\n",
    "    stream_metadata.add_dataDescriptor(\n",
    "        DataDescriptor().set_name(str(field.name)).set_type(str(field.dataType))\n",
    "    )\n",
    "stream_metadata.add_module(\n",
    "    ModuleMetadata().set_name('ppg and acl') \\\n",
    "    .set_attribute(\"url\", \"https://md2k.org\").set_author(\n",
    "        \"Md Azim Ullah\", \"mullah@memphis.edu\"))\n",
    "ds = DataStream(data=all_data._data,metadata=stream_metadata)\n",
    "CC.save_stream(ds,overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal,interpolate\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.stats import iqr\n",
    "\n",
    "def filter_data(X,\n",
    "                Fs=20,\n",
    "                low_cutoff=.5,\n",
    "                high_cutoff=3.0,\n",
    "                filter_order=5):\n",
    "    \"\"\"\n",
    "    Bandpass Filter of single channel\n",
    "\n",
    "    :param X: input data\n",
    "    :param Fs: sampling freq.\n",
    "    :param low_cutoff: low passband\n",
    "    :param high_cutoff: high passband\n",
    "    :param filter_order: no of taps in FIR filter\n",
    "\n",
    "    :return: filtered version of input data\n",
    "    \"\"\"\n",
    "    X1 = X.reshape(-1,1)\n",
    "    X1 = signal.detrend(X1,axis=0,type='constant')\n",
    "    b = signal.firls(filter_order,np.array([0,low_cutoff-.1, low_cutoff, high_cutoff ,high_cutoff+.5,Fs/2]),np.array([0, 0 ,1 ,1 ,0, 0]),\n",
    "                     np.array([100*0.02,0.02,0.02]),fs=Fs)\n",
    "    X2 = signal.convolve(X1.reshape(-1),b,mode='same')\n",
    "    return X2\n",
    "\n",
    "\n",
    "class heart_rate:\n",
    "    def __init__(self,lower_hr_range=.7,higher_hr_range=3.5,height=.03):\n",
    "        self.hr_now = 0\n",
    "        self.history_hr = []\n",
    "        self.lower_hr_range = lower_hr_range\n",
    "        self.higher_hr_range = higher_hr_range\n",
    "        self.height = height\n",
    "        self.previous = 0\n",
    "        self.step = 3\n",
    "        self.cur_time = 0\n",
    "        self.wait_time = 5*60\n",
    "    \n",
    "    def filter_frequencies(self,x_x,f_x,pxx_x):\n",
    "        x_x = np.array(x_x)\n",
    "        x_x = x_x[f_x[x_x]>self.lower_hr_range]\n",
    "        x_x = x_x[f_x[x_x]<self.higher_hr_range]\n",
    "        f_x = f_x[x_x]\n",
    "        pxx_x = pxx_x[x_x]\n",
    "        return f_x,pxx_x\n",
    "    \n",
    "    def get_peaks(self,data,fs,nperseg,nfft):\n",
    "        f_x, pxx_x = signal.welch(data,fs=fs,nperseg=nperseg,nfft=nfft)\n",
    "        f_x = f_x.reshape(-1)\n",
    "        pxx_x = pxx_x/np.max(pxx_x)\n",
    "        x_x, _ = find_peaks(pxx_x, height=self.height)\n",
    "        f,pxx = self.filter_frequencies(x_x,f_x,pxx_x)\n",
    "        ppg = np.array(list(zip(f,pxx)))\n",
    "        if len(ppg)==0:\n",
    "            return []\n",
    "        ppg = ppg[ppg[:,1].argsort()]\n",
    "        return ppg\n",
    "    \n",
    "    def get_rr_value(self,values,acc_window,time,Fs=20,nfft=12000):\n",
    "        \"\"\"\n",
    "        Get Mean RR interval\n",
    "\n",
    "        :param values: single channel ppg data\n",
    "        :param Fs: sampling frequency\n",
    "        :param nfft: FFT no. of points\n",
    "        :return: Mean RR interval Information\n",
    "        \"\"\"\n",
    "        \n",
    "#         values = filter_data(values)\n",
    "        ppg = self.get_peaks(values,Fs,values.shape[0],nfft)[-3:]\n",
    "        \n",
    "        if len(ppg)==0:\n",
    "            if time-self.cur_time>self.wait_time:\n",
    "                self.cur_time = time\n",
    "                self.history_hr = []\n",
    "                self.previous = 0\n",
    "                return [0,0]\n",
    "            \n",
    "            if len(self.history_hr)==0:\n",
    "                self.cur_time = time\n",
    "                self.previous = 0\n",
    "                return [0,0]\n",
    "            \n",
    "            elif self.previous<5:\n",
    "                self.hr_now = np.mean(self.history_hr)\n",
    "                self.history_hr.append(self.hr_now)\n",
    "                self.history_hr  =self.history_hr[-6:]\n",
    "                self.cur_time = time\n",
    "                self.previous+=1\n",
    "                return [self.hr_now,0]\n",
    "            else:\n",
    "                self.history_hr = []\n",
    "                self.previous = 0\n",
    "                return [0,0]\n",
    "        \n",
    "        aclx = list(self.get_peaks(acc_window[:,0],Fs//2,acc_window.shape[0],nfft)[-1:])\n",
    "        acly = list(self.get_peaks(acc_window[:,1],Fs//2,acc_window.shape[0],nfft)[-1:])\n",
    "        aclz = list(self.get_peaks(acc_window[:,2],Fs//2,acc_window.shape[0],nfft)[-1:])\n",
    "        acl_unwanted = np.array(aclx+acly+aclz)\n",
    "        hr_now = 0\n",
    "        if len(acl_unwanted)>0:\n",
    "            acl_unwanted = acl_unwanted[:,0]*60\n",
    "            \n",
    "            for k in range(1,len(ppg)+1):\n",
    "                hr_temp = ppg[-1*k,0]*60\n",
    "                if len(acl_unwanted[np.where((acl_unwanted>hr_temp-self.step)&(acl_unwanted<hr_temp+self.step))[0]])==0:\n",
    "                    hr_now = hr_temp\n",
    "                    break\n",
    "        else:\n",
    "            hr_now = ppg[-1,0]*60\n",
    "        \n",
    "        if hr_now==0:\n",
    "            hr_now = ppg[-1,0]*60\n",
    "        \n",
    "        \n",
    "        if time-self.cur_time>self.wait_time:\n",
    "            self.cur_time = time\n",
    "            self.history_hr = []\n",
    "            if not hr_now:\n",
    "                self.previous = 0\n",
    "                return [0,0]\n",
    "            else:\n",
    "                self.previous = 0\n",
    "                self.hr_now = hr_now\n",
    "                self.history_hr.append(self.hr_now)\n",
    "                return [self.hr_now,1]\n",
    "        \n",
    "        self.cur_time = time\n",
    "        \n",
    "        if not hr_now:\n",
    "            if len(self.history_hr)==0:\n",
    "                self.previous=0\n",
    "                return [0,0]\n",
    "            elif self.previous<=5:\n",
    "                self.hr_now = np.mean(self.history_hr)\n",
    "                self.history_hr.append(self.hr_now)\n",
    "                self.history_hr  =self.history_hr[-6:]\n",
    "                self.cur_time = time\n",
    "                self.previous+=1\n",
    "                return [self.hr_now,0]\n",
    "            else:\n",
    "                self.history_hr = []\n",
    "                self.previous = 0\n",
    "                return [0,0]\n",
    "        else:\n",
    "            if self.hr_now>0:\n",
    "                if abs(np.mean(self.history_hr)-hr_now)>10 and self.previous<=5:\n",
    "                    self.history_hr.append(hr_now)\n",
    "                    self.history_hr  =self.history_hr[-6:]\n",
    "                    self.hr_now = np.mean(self.history_hr)\n",
    "                    self.history_hr.append(self.hr_now)\n",
    "                    self.history_hr  =self.history_hr[-6:]\n",
    "                    self.previous+=1\n",
    "                    return [self.hr_now,0]\n",
    "                else:\n",
    "                    self.hr_now = hr_now\n",
    "                    self.history_hr.append(self.hr_now)\n",
    "                    self.previous = 0\n",
    "                    self.history_hr  =self.history_hr[-6:]\n",
    "                    return [self.hr_now,1]\n",
    "#                     if self.previous<=5:\n",
    "#                         self.history_hr.append(self.hr_now)\n",
    "#                         self.previous+=1\n",
    "#                         return [self.hr_now,0]\n",
    "#                     else:\n",
    "#                         self.previous=0\n",
    "#                         self.hr_now = np.mean(self.history_hr)\n",
    "#                         self.history_hr.append(self.hr_now)\n",
    "#                         self.history_hr  =self.history_hr[-6:]\n",
    "#                         return [self.hr_now,0]\n",
    "            else:\n",
    "                self.hr_now = hr_now\n",
    "                self.history_hr.append(self.hr_now)\n",
    "                self.previous = 0\n",
    "                self.history_hr  =self.history_hr[-6:]\n",
    "                return [self.hr_now,1]\n",
    "\n",
    "\n",
    "def interpolate_ppg(x,y,window_size=5,desired_sampling_freq=20):\n",
    "    df = pd.DataFrame({'time':x,'value':y})\n",
    "    df = df.sort_values('time').reset_index(drop=True)\n",
    "    f = interpolate.interp1d(df['time'].values, df['value'].values)\n",
    "    x_new = np.linspace(df['time'].min(),df['time'].max(),window_size*desired_sampling_freq)\n",
    "    return f(x_new).reshape(-1)\n",
    "\n",
    "def interpolate_acl(time,x,y,z,window_size=5,desired_sampling_freq=20):\n",
    "    df = pd.DataFrame({'time':time,'x':x,'y':y,'z':z})\n",
    "    df = df.sort_values('time').reset_index(drop=True)\n",
    "    f = interpolate.interp1d(df['time'].values, df[['x','y','z']].values,axis=0)\n",
    "    x_new = np.linspace(df['time'].min(),df['time'].max(),window_size*desired_sampling_freq)\n",
    "    return f(x_new).reshape(-1,3)\n",
    "\n",
    "\n",
    "window_size  = 5\n",
    "desired_sampling_freq = 20\n",
    "\n",
    "schema2 = StructType([\n",
    "    StructField(\"version\", IntegerType()),\n",
    "    StructField(\"user\", StringType()),\n",
    "    StructField(\"timestamp\", TimestampType()),\n",
    "    StructField(\"localtime\", TimestampType()),\n",
    "    StructField(\"start\", TimestampType()),\n",
    "    StructField(\"end\", TimestampType()),\n",
    "    StructField(\"heart_rate\", DoubleType()),\n",
    "    StructField(\"indicator\", DoubleType()),\n",
    "    StructField(\"acl_std\", DoubleType())\n",
    "])\n",
    "@pandas_udf(schema2, PandasUDFType.GROUPED_MAP)\n",
    "def get_heart_rate(df):\n",
    "    df  = df.sort_values('timestamp').reset_index(drop=True)\n",
    "    df['ppg_shape'] = df['ppg1'].apply(lambda a:len(a))\n",
    "    df = df[df['ppg_shape']>.6*100*5]\n",
    "    df['acl_shape'] = df['x'].apply(lambda a:len(a))\n",
    "    df = df[df['acl_shape']>.6*50*5]\n",
    "    df['ppg_window'] = df.apply(lambda a:interpolate_ppg(a['ppg_time'],a['ppg1'],window_size,desired_sampling_freq),axis=1)\n",
    "    df['iqr'] = df['ppg_window'].apply(lambda a:iqr(a))\n",
    "    df = df[df['iqr']>500]\n",
    "    df['acl_std'] = df.apply(lambda a:np.sqrt(np.std(a['x'])**2+np.std(a['y'])**2+np.std(a['z'])**2),axis=1)\n",
    "    df['acl_window'] = df.apply(lambda a:interpolate_acl(a['time'],a['x'],a['y'],a['z'],window_size,desired_sampling_freq),axis=1)\n",
    "    heart_rate_class = heart_rate()\n",
    "    df['heart_rate'] = df.apply(lambda a:heart_rate_class.get_rr_value(a['ppg_window'],a['acl_window'],a['time'][0]),axis=1)\n",
    "    df['indicator'] = df['heart_rate'].apply(lambda a:a[1])\n",
    "    df['heart_rate'] = df['heart_rate'].apply(lambda a:a[0])\n",
    "    df = df[df['heart_rate']>0]\n",
    "    return df[['timestamp','localtime','user','version','acl_std','heart_rate','indicator','start','end']]\n",
    "\n",
    "\n",
    "data = CC.get_stream(\"org.md2k.fossil.ppg.filtered.acl.5secs\")\n",
    "groupbycols = [\"user\",\"version\"] + ['day']\n",
    "heart_rate_data = data._data.groupBy(groupbycols).apply(get_heart_rate) \n",
    "schema = heart_rate_data.schema\n",
    "stream_metadata = Metadata()\n",
    "stream_metadata.set_name(\"org.md2k.fossil.heart.rate\").set_description('heart rate from ppg')\n",
    "for field in schema.fields:\n",
    "    stream_metadata.add_dataDescriptor(\n",
    "        DataDescriptor().set_name(str(field.name)).set_type(str(field.dataType))\n",
    "    )\n",
    "stream_metadata.add_module(\n",
    "    ModuleMetadata().set_name('heart rate from ppg') \\\n",
    "    .set_attribute(\"url\", \"https://md2k.org\").set_author(\n",
    "        \"Md Azim Ullah\", \"mullah@memphis.edu\"))\n",
    "ds = DataStream(data=heart_rate_data,metadata=stream_metadata)\n",
    "CC.save_stream(ds,overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CC.get_stream(\"org.md2k.fossil.heart.rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling Routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_rate_data = CC.get_stream(\"org.md2k.fossil.heart.rate\").drop('timestamp','localtime','version','acl_std')\n",
    "acl_data = CC.get_stream(\"org.md2k.fossil.acl.5secs\")\n",
    "\n",
    "all_data = acl_data.join(heart_rate_data,on=['start','end','user'],how='left')\n",
    "\n",
    "all_data = all_data.withColumn('day',F.date_format('localtime',\"yyyyMMdd\"))\n",
    "groupbycols = [\"user\",\"version\"] + ['day']\n",
    "\n",
    "schema2 = StructType([\n",
    "    StructField(\"version\", IntegerType()),\n",
    "    StructField(\"user\", StringType()),\n",
    "    StructField(\"timestamp\", TimestampType()),\n",
    "    StructField(\"localtime\", TimestampType()),\n",
    "    StructField(\"heart_rate\", DoubleType()),\n",
    "    StructField(\"indicator\", DoubleType()),\n",
    "    StructField(\"acl_std\", DoubleType()),\n",
    "    StructField(\"day\", StringType())\n",
    "])\n",
    "@pandas_udf(schema2, PandasUDFType.GROUPED_MAP)\n",
    "def get_acl_std(df):\n",
    "    df['acl_shape'] = df['x'].apply(lambda a:len(a))\n",
    "    df = df[df['acl_shape']>.2*50*5]\n",
    "    df['acl_std'] = df.apply(lambda a:np.sqrt(np.std(a['x'])**2+np.std(a['y'])**2+np.std(a['z'])**2),axis=1)\n",
    "    return df[['acl_std','timestamp','localtime','version','heart_rate','indicator','user','day']]\n",
    "\n",
    "heart_rate_data = all_data._data.groupBy(groupbycols).apply(get_acl_std)\n",
    "schema = heart_rate_data.schema\n",
    "stream_metadata = Metadata()\n",
    "stream_metadata.set_name(\"org.md2k.fossil.heart.rate.acl.std.5.secs\").set_description('heart rate from ppg')\n",
    "for field in schema.fields:\n",
    "    stream_metadata.add_dataDescriptor(\n",
    "        DataDescriptor().set_name(str(field.name)).set_type(str(field.dataType))\n",
    "    )\n",
    "stream_metadata.add_module(\n",
    "    ModuleMetadata().set_name('heart rate from ppg combined with acl standard deviation in 5 seconds windows') \\\n",
    "    .set_attribute(\"url\", \"https://md2k.org\").set_author(\n",
    "        \"Md Azim Ullah\", \"mullah@memphis.edu\"))\n",
    "ds = DataStream(data=heart_rate_data,metadata=stream_metadata)\n",
    "CC.save_stream(ds,overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worn_data = CC.get_stream('worn--org.md2k.watch--fossil_watch_sport').drop('version','localtime')\n",
    "\n",
    "window_size = 60\n",
    "win = F.window(\"timestamp\", windowDuration=str(window_size)+' seconds',startTime='0 seconds')\n",
    "groupbycols = [\"user\"] + [win]\n",
    "worn_data = worn_data._data.groupBy(groupbycols).agg(F.mean('worn')).withColumnRenamed('avg(worn)','worn')\n",
    "\n",
    "data = CC.get_stream(\"org.md2k.fossil.heart.rate.acl.std.5.secs\")\n",
    "window_size = 60\n",
    "win = F.window(\"timestamp\", windowDuration=str(window_size)+' seconds',startTime='0 seconds')\n",
    "groupbycols = [\"user\",\"version\",'day'] + [win]\n",
    "\n",
    "data = data._data.groupBy(groupbycols).agg(F.collect_list('acl_std'),F.collect_list('heart_rate'),F.collect_list('timestamp'),F.collect_list('localtime'))\n",
    "\n",
    "data = data.withColumnRenamed('collect_list(heart_rate)','heart_rate')\n",
    "data = data.withColumnRenamed('collect_list(timestamp)','timestamp')\n",
    "data = data.withColumnRenamed('collect_list(localtime)','localtime')\n",
    "data = data.withColumnRenamed('collect_list(acl_std)','acl_std')\n",
    "data = data.withColumn('timestamp',F.col('timestamp').getItem(0))\n",
    "data = data.withColumn('localtime',F.col('localtime').getItem(0))\n",
    "\n",
    "data = data.join(worn_data,on=['user','window'],how='left').drop('window')\n",
    "\n",
    "data = data.withColumn('time',F.col('timestamp').cast('double'))\n",
    "\n",
    "# import pickle\n",
    "# pickle.dump(data.toPandas(),open('../data/temp.p','wb'))\n",
    "\n",
    "class Sample:\n",
    "    def __init__(self):\n",
    "        self.acl_std = []\n",
    "        self.cur_time = 0\n",
    "        self.last_time = 0\n",
    "        self.wait_time = 10*60\n",
    "        self.must_sample = 20*60\n",
    "        self.sample_decision = 1\n",
    "        self.good_time = -np.inf\n",
    "        self.count = 0\n",
    "        self.last_sampled = 0\n",
    "        self.threshold = 0.015\n",
    "        self.min_threshold = .01\n",
    "        \n",
    "    def get_decision(self,acl_std,cur_time,hr):\n",
    "        temp = self.sample_decision\n",
    "        self.last_time = self.cur_time\n",
    "        self.cur_time = cur_time\n",
    "        self.acl_std.append(acl_std)\n",
    "        self.acl_std = np.array(self.acl_std[-3:])\n",
    "        if self.sample_decision==1:\n",
    "            if len(hr)>0:\n",
    "                self.count = 0\n",
    "                self.good_time = self.last_time\n",
    "            else:\n",
    "                self.count+=1\n",
    "#         elif len(self.acl_std[self.acl_std>=self.min_threshold])>1:\n",
    "#             self.count -= .5 \n",
    "        elif len(self.acl_std[self.acl_std>=self.threshold])>1:\n",
    "            self.count -= 1 \n",
    "        \n",
    "        \n",
    "                \n",
    "        if  self.cur_time-self.last_sampled>=self.must_sample:\n",
    "            self.count = 0\n",
    "            self.sample_decision = 1\n",
    "        \n",
    "        elif self.cur_time - self.last_time>=self.wait_time:\n",
    "            self.count = 0\n",
    "            self.acl_std = self.acl_std[-1:]\n",
    "            self.sample_decision = 1\n",
    "        \n",
    "        elif self.good_time==self.last_time:\n",
    "            self.sample_decision = 1\n",
    "        \n",
    "        elif len(self.acl_std)<2:\n",
    "            self.sample_decision = 1\n",
    "        \n",
    "        elif self.count>=3:\n",
    "#             self.count-=.5\n",
    "            self.sample_decision = 0\n",
    "        \n",
    "        elif len(self.acl_std[self.acl_std>=self.threshold])>2:\n",
    "            if acl_std>3:\n",
    "                if self.count>3:\n",
    "                    self.sample_decision = 0\n",
    "                else:\n",
    "                    self.sample_decision = 1\n",
    "            else:\n",
    "                self.sample_decision = 1\n",
    "        elif len(self.acl_std[self.acl_std<self.threshold])>2:\n",
    "            if self.count<=3:\n",
    "                self.sample_decision = 1\n",
    "            else:\n",
    "                self.sample_decision = 0\n",
    "    \n",
    "        \n",
    "        else:\n",
    "            self.sample_decision = 0\n",
    "        \n",
    "        if self.sample_decision==1:\n",
    "            self.last_sampled = self.cur_time \n",
    "        self.acl_std = list(self.acl_std)\n",
    "        return self.sample_decision\n",
    "\n",
    "        \n",
    "schema2 = StructType([\n",
    "    StructField(\"version\", IntegerType()),\n",
    "    StructField(\"user\", StringType()),\n",
    "    StructField(\"timestamp\", TimestampType()),\n",
    "    StructField(\"localtime\", TimestampType()),\n",
    "    StructField(\"heart_rate\", DoubleType()),\n",
    "    StructField(\"sample_decision\", DoubleType()),\n",
    "    StructField(\"acl_std\", DoubleType()),\n",
    "    StructField(\"day\", StringType()),\n",
    "    StructField(\"worn\", DoubleType())\n",
    "    \n",
    "])\n",
    "from scipy.stats import mode\n",
    "@pandas_udf(schema2, PandasUDFType.GROUPED_MAP)\n",
    "def get_sampling_decision(df):\n",
    "    df['acl_shape'] = df['acl_std'].apply(lambda a:len(a))\n",
    "    df = df[df['acl_shape']>15]\n",
    "    df['acl_std'] = df['acl_std'].apply(lambda a:np.nanpercentile(a,20))\n",
    "    sampling_decision = [1]\n",
    "    df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "    print(df['day'][0])\n",
    "    print('--'*20)\n",
    "    sample_class = Sample()\n",
    "    for i,row in df.iterrows():\n",
    "        if i==len(df)-1:\n",
    "            continue\n",
    "        if sampling_decision[-1]==1:\n",
    "            hr = [a for a in row['heart_rate'] if a is not None or not np.isnan(a)]\n",
    "        else:\n",
    "            hr = []\n",
    "        sampling_decision.append(sample_class.get_decision(row['acl_std'],row['time'],hr))\n",
    "    df['sample_decision'] = sampling_decision\n",
    "    df['heart_rate'] = df['heart_rate'].apply(lambda a:np.median(a) if len(a)>0 else 0)\n",
    "    return df[['user','day','version','localtime','timestamp','acl_std','heart_rate','sample_decision','worn']]\n",
    "    \n",
    "heart_rate_data = data.groupBy(['user','day','version']).apply(get_sampling_decision)\n",
    "schema = heart_rate_data.schema\n",
    "stream_metadata = Metadata()\n",
    "stream_metadata.set_name(\"org.md2k.fossil.heart.rate.acl.std.sample.decision.60.secs\").set_description('Sampling Routine')\n",
    "for field in schema.fields:\n",
    "    stream_metadata.add_dataDescriptor(\n",
    "        DataDescriptor().set_name(str(field.name)).set_type(str(field.dataType))\n",
    "    )\n",
    "stream_metadata.add_module(\n",
    "    ModuleMetadata().set_name('Sampling Routine with minute level heart rate') \\\n",
    "    .set_attribute(\"url\", \"https://md2k.org\").set_author(\n",
    "        \"Md Azim Ullah\", \"mullah@memphis.edu\"))\n",
    "ds = DataStream(data=heart_rate_data,metadata=stream_metadata)\n",
    "CC.save_stream(ds,overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_rate_data = CC.get_stream(\"org.md2k.fossil.heart.rate.acl.std.sample.decision.60.secs\")\n",
    "\n",
    "heart_rate_data = heart_rate_data.withColumn('time',F.col('timestamp').cast('double'))\n",
    "heart_rate_data = heart_rate_data.withColumn('day',F.date_format('localtime',\"yyyyMMdd\"))\n",
    "\n",
    "log_data = CC.get_stream('moods_logs')\n",
    "# log_data = log_data.withColumn('time', F.col('timestamp').cast('timestamp')-\"5 hours\")\n",
    "log_data = log_data.withColumn('day',F.date_format('timestamp',\"yyyyMMdd\")).toPandas()\n",
    "# log_data = log_data.withColumn('time',F.col('timestamp').cast('double')).toPandas()\n",
    "log_data['ind'] = log_data['message'].apply(lambda a:a.find('ACTIVITY INPUT'))\n",
    "log_data = log_data[log_data.ind>-1]\n",
    "\n",
    "log_data['message'] = log_data['message'].apply(lambda a:a[17:-1])\n",
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "log_data['timestamp'] = log_data['timestamp'].apply(lambda x:parse(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data1 = heart_rate_data.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data1['time'] = temp_data1['timestamp'].apply(lambda a:a.timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_data['time'] = log_data['timestamp'].apply(lambda a:a.timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "count = 0\n",
    "plt.rcParams.update({'font.size':25})\n",
    "import matplotlib.pyplot as plt\n",
    "for user in np.unique(temp_data1['user'].values):\n",
    "    for day in np.unique(temp_data1['day'].values):\n",
    "        temp_data = temp_data1[(temp_data1.day==day)&(temp_data1.user==user)].sort_values('timestamp').reset_index(drop=True)\n",
    "        temp_data.fillna(-1,inplace=True)\n",
    "        temp_log = log_data[(log_data.uuid==user)&(log_data.time>=temp_data.time.min())&(log_data.time<=temp_data.time.max())]\n",
    "        print(temp_log.shape)\n",
    "        if temp_data.shape[0]<10:\n",
    "            continue\n",
    "        temp_data['localtime'] = temp_data['localtime'].apply(lambda a:a.strftime(\"%H:%M\"))\n",
    "        fig,ax = plt.subplots(4,1,figsize=(20,15),sharex=True)\n",
    "        \n",
    "        ax[0].plot(temp_data[temp_data['heart_rate']>0]['timestamp'],temp_data[temp_data['heart_rate']>0]['heart_rate'],'--*')\n",
    "        ax[1].bar(temp_data['timestamp'],temp_data['acl_std'],0.001,color='red')\n",
    "        ax[2].plot(temp_data['timestamp'],temp_data['sample_decision'],'--o',color='green')\n",
    "        ax[0].set_title('Heart Rate')\n",
    "        ax[1].set_title('20th percentile of accelerometer standard deviation')\n",
    "        ax[2].set_title('Sampling Decisions')\n",
    "        ax[3].set_title('Logs and Google Wear')\n",
    "        \n",
    "        \n",
    "#         ax[0].set_title('Sampling Decision')\n",
    "    #     plt.show()\n",
    "    #     plt.figure(figsize=(10,10))\n",
    "#         ax[1].bar(temp_data['timestamp'],temp_data['std'],.001,color='red')\n",
    "#         ax[0].set_title(temp_data['day'][0])\n",
    "        plt.xlabel(temp_data['user'][0]+'---'+temp_data['day'][0])\n",
    "#         ax[2].bar(temp_data['timestamp'],iqr_list,.001,color='green')\n",
    "#         ax[2].set_title('Status of PPG Data')\n",
    "        \n",
    "#         plt.setp(ax[2],xticks= temp_log['timestamp'],xticklabels=temp_log['timestamp'])\n",
    "        for i,row in temp_log.iterrows():\n",
    "            ax[3].text(row['timestamp'], 0.05, row['message'],fontsize=15,rotation=90)\n",
    "        ax[3].vlines(temp_data[temp_data.worn>-1]['timestamp'],0,1)\n",
    "        plt.setp(ax[3],xticks= np.array(temp_data['timestamp'])[np.arange(0,temp_data.shape[0],40)],xticklabels=np.array(temp_data['localtime'])[np.arange(0,temp_data.shape[0],40)])\n",
    "        plt.xticks(rotation=60)\n",
    "        from datetime import timedelta\n",
    "#         ax[3].set_xlim([temp_data['timestamp'].min()-timedelta(hours=1),temp_data['timestamp'].max()+timedelta(hours=1)])\n",
    "        plt.savefig('../images_sampling/'+str(count)+'.pdf',dps=1e6)\n",
    "        count+=1\n",
    "        plt.show()\n",
    "        print('-'*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = heart_rate_data._data.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_rate_data.sort(F.col('timestamp')).show(8000,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_rate_data.show(80000,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data1.user.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/mullah/moods/images_sampling.zip'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutil.make_archive('../images_sampling/','zip','../images_sampling/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "High Performance CC3.3",
   "language": "python",
   "name": "cc33_high_performance"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
