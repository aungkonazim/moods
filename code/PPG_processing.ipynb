{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cerebralcortex.util.helper_methods import get_study_names\n",
    "sn = get_study_names(\"/home/jupyter/cc3_conf/\")\n",
    "print(sn)\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import StructField, StructType, DoubleType,MapType, StringType,ArrayType, FloatType, TimestampType, IntegerType\n",
    "from pyspark.sql.functions import minute, second, mean, window\n",
    "from pyspark.sql import functions as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from cerebralcortex.core.datatypes import DataStream\n",
    "from cerebralcortex.core.metadata_manager.stream.metadata import Metadata, DataDescriptor, \\\n",
    "ModuleMetadata\n",
    "from typing import List\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "import pandas as pd\n",
    "from cerebralcortex import Kernel\n",
    "from pyspark.sql import functions as F\n",
    "CC = Kernel(\"/home/jupyter/cc3_moods_conf/\", study_name='moods')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bandpass filter the PPG data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import StructField, StructType, DoubleType, StringType, TimestampType, IntegerType\n",
    "import numpy as np\n",
    "from cerebralcortex.core.datatypes import DataStream\n",
    "from cerebralcortex.core.metadata_manager.stream.metadata import Metadata, DataDescriptor, \\\n",
    "    ModuleMetadata\n",
    "from scipy import signal\n",
    "def filter_data(X,\n",
    "                Fs=100,\n",
    "                low_cutoff=.4,\n",
    "                high_cutoff=3.0,\n",
    "                filter_order=65):\n",
    "    \"\"\"\n",
    "    Bandpass Filter of single channel\n",
    "\n",
    "    :param X: input data\n",
    "    :param Fs: sampling freq.\n",
    "    :param low_cutoff: low passband\n",
    "    :param high_cutoff: high passband\n",
    "    :param filter_order: no of taps in FIR filter\n",
    "\n",
    "    :return: filtered version of input data\n",
    "    \"\"\"\n",
    "    X1 = X.reshape(-1,1)\n",
    "    X1 = signal.detrend(X1,axis=0,type='constant')\n",
    "    b = signal.firls(filter_order,np.array([0,low_cutoff-.1, low_cutoff, high_cutoff ,high_cutoff+.5,Fs/2]),np.array([0, 0 ,1 ,1 ,0, 0]),\n",
    "                     np.array([100*0.02,0.02,0.02]),fs=Fs)\n",
    "    X2 = signal.convolve(X1.reshape(-1),b,mode='same')\n",
    "    return X2\n",
    "\n",
    "\n",
    "from scipy.signal import butter, lfilter\n",
    "\n",
    "\n",
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return b, a\n",
    "\n",
    "\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    y = lfilter(b, a, data)\n",
    "    return y\n",
    "\n",
    "def get_metadata(data,\n",
    "                 wrist='left',\n",
    "                 sensor_name='motionsensehrv',\n",
    "                 ppg_columns=('red','infrared','green'),\n",
    "                 acl_columns=('aclx','acly','aclz')):\n",
    "    \"\"\"\n",
    "    :param data: input stream\n",
    "    :param wrist: which wrist the data was collected from\n",
    "    :param sensor_name: name of sensor\n",
    "    :param ppg_columns: columns in the input dataframe referring to multiple ppg channels\n",
    "    :param acl_columns: columns in the input dataframe referring to accelerometer channels\n",
    "\n",
    "    :return: metadata of output stream\n",
    "    \"\"\"\n",
    "    stream_name = \"org.md2k.\"+str(sensor_name)+\".\"+str(wrist)+\".wrist.bandpass.filtered\"\n",
    "    print(stream_name)\n",
    "    stream_metadata = Metadata()\n",
    "    stream_metadata.set_name(stream_name).set_description(\"Bandpass Filtered PPG data\") \\\n",
    "        .add_dataDescriptor(DataDescriptor().set_name(\"timestamp\").set_type(\"datetime\")) \\\n",
    "        .add_dataDescriptor(DataDescriptor().set_name(\"localtime\").set_type(\"datetime\")) \\\n",
    "        .add_dataDescriptor(DataDescriptor().set_name(\"version\").set_type(\"int\")) \\\n",
    "        .add_dataDescriptor(DataDescriptor().set_name(\"user\").set_type(\"string\"))\n",
    "\n",
    "    for c in ppg_columns:\n",
    "        stream_metadata.add_dataDescriptor(DataDescriptor().set_name(c).set_type(\"double\").set_attribute(\"description\",\n",
    "                                                                                                    \"ppg channel \"+c))\n",
    "    for c in acl_columns:\n",
    "        stream_metadata.add_dataDescriptor(DataDescriptor().set_name(c).set_type(\"double\").set_attribute(\"description\",\n",
    "                                                                                            \"accelerometer channel \"+c))\n",
    "\n",
    "    stream_metadata.add_module(\n",
    "        ModuleMetadata().set_name(\"ecg data quality\").set_attribute(\"url\", \"http://md2k.org/\").set_author(\n",
    "            \"Md Azim Ullah\", \"mullah@memphis.edu\"))\n",
    "    return stream_metadata\n",
    "\n",
    "\n",
    "def bandpass_filter(\n",
    "                   data,\n",
    "                   Fs = 25,\n",
    "                   low_cutoff = 0.4,\n",
    "                   high_cutoff = 3.0,\n",
    "                   filter_order = 65,\n",
    "                   ppg_columns=('red','infrared','green'),\n",
    "                   acl_columns=('aclx','acly','aclz'),\n",
    "                   wrist='left',\n",
    "                   sensor_name='motionsensehrv'):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    :param data: PPG & ACL data stream\n",
    "    :param Fs: sampling frequency\n",
    "    :param low_cutoff: minimum frequency of pass band\n",
    "    :param high_cutoff: Maximum Frequency of pass band\n",
    "    :param filter_order: no. of taps in FIR filter\n",
    "    :param ppg_columns: columns in the input dataframe referring to multiple ppg channels\n",
    "    :param acl_columns: columns in the input dataframe referring to accelerometer channels\n",
    "    :param wrist: which wrist the data was collected from\n",
    "    :param sensor_name: name of sensor\n",
    "\n",
    "    :return: Bandpass filtered version of input PPG data\n",
    "    \"\"\"\n",
    "\n",
    "    ## check if all columns exist\n",
    "\n",
    "    default_columns = ['user','version','localtime','timestamp']\n",
    "    required_columns = default_columns+acl_columns+ppg_columns\n",
    "    if len(set(required_columns)-set(data.columns))>0:\n",
    "        raise Exception(\"Columns missing in input dataframe! \" + str(list(set(required_columns)-set(data.columns))))\n",
    "\n",
    "    ## select the columns from input dataframe\n",
    "\n",
    "    data = data.select(*required_columns)\n",
    "\n",
    "    ## udf\n",
    "\n",
    "    default_schema = [StructField(\"timestamp\", TimestampType()),\n",
    "                      StructField(\"localtime\", TimestampType()),\n",
    "                      StructField(\"version\", IntegerType()),\n",
    "                      StructField(\"user\", StringType())]\n",
    "    schema = StructType(default_schema+[StructField(c, DoubleType()) for c in list(ppg_columns)+list(acl_columns)])\n",
    "    @pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "    def ppg_bandpass(data):\n",
    "        data = data.sort_values('timestamp').reset_index(drop=True)\n",
    "        for c in ppg_columns:\n",
    "#             data[c] = filter_data(data[c].values,Fs=Fs,low_cutoff=low_cutoff,high_cutoff=high_cutoff,filter_order=filter_order)\n",
    "            data[c] = butter_bandpass_filter(data[c].values, low_cutoff, high_cutoff, fs=Fs, order=5)\n",
    "        return data\n",
    "\n",
    "    ## steps\n",
    "    ppg_bandpass_filtered = data.compute(ppg_bandpass,windowDuration=60*60*10,startTime='0 seconds')\n",
    "    output_data = ppg_bandpass_filtered._data\n",
    "    ds = DataStream(data=output_data,metadata=get_metadata(data,wrist=wrist,sensor_name=sensor_name,\n",
    "                                                           ppg_columns=ppg_columns,acl_columns=acl_columns))\n",
    "    return ds\n",
    "\n",
    "data = CC.get_stream('ppg--org.md2k.watch--fossil_watch_sport')\n",
    "from datetime import datetime\n",
    "data = data.filter(F.col('localtime')>datetime(2020,12,1))\n",
    "data  = data.withColumn('day',F.date_format('localtime',\"YYYYMMdd\"))\n",
    "filtered_data = bandpass_filter(\n",
    "                   data,\n",
    "                   Fs = 100,\n",
    "                   low_cutoff = 0.4,\n",
    "                   high_cutoff = 3.0,\n",
    "                   filter_order = 65,\n",
    "                   ppg_columns=['ppg1'],\n",
    "                   acl_columns=[],\n",
    "                   wrist='left',\n",
    "                   sensor_name='fossil')\n",
    "CC.save_stream(filtered_data,overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aggregate_second_level_data(data,input_columns,window_size,sliding_size,stream_name,description):\n",
    "    import numpy as np\n",
    "    default_schema = [\n",
    "        StructField(\"version\", IntegerType()),\n",
    "        StructField(\"user\", StringType()),\n",
    "        StructField(\"start\", TimestampType()),\n",
    "        StructField(\"end\", TimestampType()),\n",
    "        StructField(\"timestamp\", TimestampType()),\n",
    "        StructField(\"localtime\", TimestampType())\n",
    "    ]\n",
    "    new_schema = [StructField(a, ArrayType(DoubleType())) for a in input_columns]\n",
    "    schema = StructType(default_schema+new_schema)\n",
    "    @pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "    def compute_window(key,data1):\n",
    "        data1 = data1.sort_values('timestamp').reset_index(drop=True)\n",
    "        temp = [data1.version.values[0],\n",
    "               data1.user.values[0],\n",
    "               key[2]['start'],\n",
    "               key[2]['end'],\n",
    "               data1['timestamp'].values[0],\n",
    "               data1['localtime'].values[0]]\n",
    "        for a in input_columns:\n",
    "            temp.append(data1[a].values)\n",
    "        return pd.DataFrame([temp],columns=['version','user','start','end','timestamp','localtime']+input_columns)\n",
    "    win = F.window(\"timestamp\", windowDuration=str(window_size)+' seconds',startTime='0 seconds',slideDuration=str(sliding_size)+' seconds')\n",
    "    groupbycols = [\"user\",\"version\"] + [win]\n",
    "    activity_data = data._data.groupBy(groupbycols).apply(compute_window)\n",
    "    schema = activity_data.schema\n",
    "    stream_metadata = Metadata()\n",
    "    stream_metadata.set_name(stream_name).set_description(description)\n",
    "    for field in schema.fields:\n",
    "        stream_metadata.add_dataDescriptor(\n",
    "            DataDescriptor().set_name(str(field.name)).set_type(str(field.dataType))\n",
    "        )\n",
    "    stream_metadata.add_module(\n",
    "        ModuleMetadata().set_name(description) \\\n",
    "        .set_attribute(\"url\", \"https://md2k.org\").set_author(\n",
    "            \"Md Azim Ullah\", \"mullah@memphis.edu\"))\n",
    "#     stream_metadata.is_valid()\n",
    "    ds = DataStream(data=activity_data,metadata=stream_metadata)\n",
    "    return ds\n",
    "\n",
    "\n",
    "ppg_data  = CC.get_stream('org.md2k.fossil.left.wrist.bandpass.filtered')\n",
    "ppg_data = ppg_data.withColumn('time',F.col('timestamp').cast('double'))\n",
    "from datetime import datetime\n",
    "ppg_data = ppg_data.filter(F.col('localtime')>datetime(2020,12,1))\n",
    "ppg_5_secs_data = get_aggregate_second_level_data(ppg_data,['ppg1','time'],5,2,\"org.md2k.fossil.ppg.filtered.5secs\",'filtered ppg in 5 seconds')\n",
    "CC.save_stream(ppg_5_secs_data,overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acl_data = CC.get_stream('accelerometer--org.md2k.watch--fossil_watch_sport')\n",
    "acl_data = acl_data.withColumn('time',F.col('timestamp').cast('double'))\n",
    "acl_data = acl_data.filter(F.col('localtime')>datetime(2020,12,1))\n",
    "acl_5_secs_data = get_aggregate_second_level_data(acl_data,['time','x','y','z'],5,2,\"org.md2k.fossil.acl.5secs\",'acl in 5 seconds')\n",
    "CC.save_stream(acl_5_secs_data,overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppg_data = CC.get_stream(\"org.md2k.fossil.ppg.filtered.5secs\").drop('timestamp','localtime','version').withColumnRenamed('time','ppg_time')\n",
    "\n",
    "acl_data = CC.get_stream(\"org.md2k.fossil.acl.5secs\")\n",
    "\n",
    "# ppg_data.printSchema(), acl_data.printSchema()\n",
    "all_data = acl_data.join(ppg_data,on=['start','end','user'],how='inner')\n",
    "\n",
    "all_data = all_data.withColumn('day',F.date_format('localtime',\"yyyyMMdd\"))\n",
    "\n",
    "schema = all_data._data.schema\n",
    "stream_metadata = Metadata()\n",
    "stream_metadata.set_name(\"org.md2k.fossil.ppg.filtered.acl.5secs\").set_description('ppg and acl')\n",
    "for field in schema.fields:\n",
    "    stream_metadata.add_dataDescriptor(\n",
    "        DataDescriptor().set_name(str(field.name)).set_type(str(field.dataType))\n",
    "    )\n",
    "stream_metadata.add_module(\n",
    "    ModuleMetadata().set_name('ppg and acl') \\\n",
    "    .set_attribute(\"url\", \"https://md2k.org\").set_author(\n",
    "        \"Md Azim Ullah\", \"mullah@memphis.edu\"))\n",
    "ds = DataStream(data=all_data._data,metadata=stream_metadata)\n",
    "CC.save_stream(ds,overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal,interpolate\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.stats import iqr\n",
    "\n",
    "def filter_data(X,\n",
    "                Fs=20,\n",
    "                low_cutoff=.5,\n",
    "                high_cutoff=3.0,\n",
    "                filter_order=5):\n",
    "    \"\"\"\n",
    "    Bandpass Filter of single channel\n",
    "\n",
    "    :param X: input data\n",
    "    :param Fs: sampling freq.\n",
    "    :param low_cutoff: low passband\n",
    "    :param high_cutoff: high passband\n",
    "    :param filter_order: no of taps in FIR filter\n",
    "\n",
    "    :return: filtered version of input data\n",
    "    \"\"\"\n",
    "    X1 = X.reshape(-1,1)\n",
    "    X1 = signal.detrend(X1,axis=0,type='constant')\n",
    "    b = signal.firls(filter_order,np.array([0,low_cutoff-.1, low_cutoff, high_cutoff ,high_cutoff+.5,Fs/2]),np.array([0, 0 ,1 ,1 ,0, 0]),\n",
    "                     np.array([100*0.02,0.02,0.02]),fs=Fs)\n",
    "    X2 = signal.convolve(X1.reshape(-1),b,mode='same')\n",
    "    return X2\n",
    "\n",
    "\n",
    "class heart_rate:\n",
    "    def __init__(self,lower_hr_range=.7,higher_hr_range=3.5,height=.03):\n",
    "        self.hr_now = 0\n",
    "        self.history_hr = []\n",
    "        self.lower_hr_range = lower_hr_range\n",
    "        self.higher_hr_range = higher_hr_range\n",
    "        self.height = height\n",
    "        self.previous = 0\n",
    "        self.step = 3\n",
    "        self.cur_time = 0\n",
    "        self.wait_time = 5*60\n",
    "    \n",
    "    def filter_frequencies(self,x_x,f_x,pxx_x):\n",
    "        x_x = np.array(x_x)\n",
    "        x_x = x_x[f_x[x_x]>self.lower_hr_range]\n",
    "        x_x = x_x[f_x[x_x]<self.higher_hr_range]\n",
    "        f_x = f_x[x_x]\n",
    "        pxx_x = pxx_x[x_x]\n",
    "        return f_x,pxx_x\n",
    "    \n",
    "    def get_peaks(self,data,fs,nperseg,nfft):\n",
    "        f_x, pxx_x = signal.welch(data,fs=fs,nperseg=nperseg,nfft=nfft)\n",
    "        f_x = f_x.reshape(-1)\n",
    "        pxx_x = pxx_x/np.max(pxx_x)\n",
    "        x_x, _ = find_peaks(pxx_x, height=self.height)\n",
    "        f,pxx = self.filter_frequencies(x_x,f_x,pxx_x)\n",
    "        ppg = np.array(list(zip(f,pxx)))\n",
    "        if len(ppg)==0:\n",
    "            return []\n",
    "        ppg = ppg[ppg[:,1].argsort()]\n",
    "        return ppg\n",
    "    \n",
    "    def get_rr_value(self,values,acc_window,time,Fs=20,nfft=12000):\n",
    "        \"\"\"\n",
    "        Get Mean RR interval\n",
    "\n",
    "        :param values: single channel ppg data\n",
    "        :param Fs: sampling frequency\n",
    "        :param nfft: FFT no. of points\n",
    "        :return: Mean RR interval Information\n",
    "        \"\"\"\n",
    "        \n",
    "#         values = filter_data(values)\n",
    "        ppg = self.get_peaks(values,Fs,values.shape[0],nfft)[-3:]\n",
    "        \n",
    "        if len(ppg)==0:\n",
    "            if time-self.cur_time>self.wait_time:\n",
    "                self.cur_time = time\n",
    "                self.history_hr = []\n",
    "                self.previous = 0\n",
    "                return [0,0]\n",
    "            \n",
    "            if len(self.history_hr)==0:\n",
    "                self.cur_time = time\n",
    "                self.previous = 0\n",
    "                return [0,0]\n",
    "            \n",
    "            elif self.previous<5:\n",
    "                self.hr_now = np.mean(self.history_hr)\n",
    "                self.history_hr.append(self.hr_now)\n",
    "                self.history_hr  =self.history_hr[-6:]\n",
    "                self.cur_time = time\n",
    "                self.previous+=1\n",
    "                return [self.hr_now,0]\n",
    "            else:\n",
    "                self.history_hr = []\n",
    "                self.previous = 0\n",
    "                return [0,0]\n",
    "        \n",
    "        aclx = list(self.get_peaks(acc_window[:,0],Fs//2,acc_window.shape[0],nfft)[-1:])\n",
    "        acly = list(self.get_peaks(acc_window[:,1],Fs//2,acc_window.shape[0],nfft)[-1:])\n",
    "        aclz = list(self.get_peaks(acc_window[:,2],Fs//2,acc_window.shape[0],nfft)[-1:])\n",
    "        acl_unwanted = np.array(aclx+acly+aclz)\n",
    "        hr_now = 0\n",
    "        if len(acl_unwanted)>0:\n",
    "            acl_unwanted = acl_unwanted[:,0]*60\n",
    "            \n",
    "            for k in range(1,len(ppg)+1):\n",
    "                hr_temp = ppg[-1*k,0]*60\n",
    "                if len(acl_unwanted[np.where((acl_unwanted>hr_temp-self.step)&(acl_unwanted<hr_temp+self.step))[0]])==0:\n",
    "                    hr_now = hr_temp\n",
    "                    break\n",
    "        else:\n",
    "            hr_now = ppg[-1,0]*60\n",
    "        \n",
    "        if hr_now==0:\n",
    "            hr_now = ppg[-1,0]*60\n",
    "        \n",
    "        \n",
    "        if time-self.cur_time>self.wait_time:\n",
    "            self.cur_time = time\n",
    "            self.history_hr = []\n",
    "            if not hr_now:\n",
    "                self.previous = 0\n",
    "                return [0,0]\n",
    "            else:\n",
    "                self.previous = 0\n",
    "                self.hr_now = hr_now\n",
    "                self.history_hr.append(self.hr_now)\n",
    "                return [self.hr_now,1]\n",
    "        \n",
    "        self.cur_time = time\n",
    "        \n",
    "        if not hr_now:\n",
    "            if len(self.history_hr)==0:\n",
    "                self.previous=0\n",
    "                return [0,0]\n",
    "            elif self.previous<=5:\n",
    "                self.hr_now = np.mean(self.history_hr)\n",
    "                self.history_hr.append(self.hr_now)\n",
    "                self.history_hr  =self.history_hr[-6:]\n",
    "                self.cur_time = time\n",
    "                self.previous+=1\n",
    "                return [self.hr_now,0]\n",
    "            else:\n",
    "                self.history_hr = []\n",
    "                self.previous = 0\n",
    "                return [0,0]\n",
    "        else:\n",
    "            if self.hr_now>0:\n",
    "                if abs(np.mean(self.history_hr)-hr_now)>10 and self.previous<=5:\n",
    "                    self.history_hr.append(hr_now)\n",
    "                    self.history_hr  =self.history_hr[-6:]\n",
    "                    self.hr_now = np.mean(self.history_hr)\n",
    "                    self.history_hr.append(self.hr_now)\n",
    "                    self.history_hr  =self.history_hr[-6:]\n",
    "                    self.previous+=1\n",
    "                    return [self.hr_now,0]\n",
    "                else:\n",
    "                    self.hr_now = hr_now\n",
    "                    self.history_hr.append(self.hr_now)\n",
    "                    self.previous = 0\n",
    "                    self.history_hr  =self.history_hr[-6:]\n",
    "                    return [self.hr_now,1]\n",
    "#                     if self.previous<=5:\n",
    "#                         self.history_hr.append(self.hr_now)\n",
    "#                         self.previous+=1\n",
    "#                         return [self.hr_now,0]\n",
    "#                     else:\n",
    "#                         self.previous=0\n",
    "#                         self.hr_now = np.mean(self.history_hr)\n",
    "#                         self.history_hr.append(self.hr_now)\n",
    "#                         self.history_hr  =self.history_hr[-6:]\n",
    "#                         return [self.hr_now,0]\n",
    "            else:\n",
    "                self.hr_now = hr_now\n",
    "                self.history_hr.append(self.hr_now)\n",
    "                self.previous = 0\n",
    "                self.history_hr  =self.history_hr[-6:]\n",
    "                return [self.hr_now,1]\n",
    "\n",
    "\n",
    "def interpolate_ppg(x,y,window_size=5,desired_sampling_freq=20):\n",
    "    df = pd.DataFrame({'time':x,'value':y})\n",
    "    df = df.sort_values('time').reset_index(drop=True)\n",
    "    f = interpolate.interp1d(df['time'].values, df['value'].values)\n",
    "    x_new = np.linspace(df['time'].min(),df['time'].max(),window_size*desired_sampling_freq)\n",
    "    return f(x_new).reshape(-1)\n",
    "\n",
    "def interpolate_acl(time,x,y,z,window_size=5,desired_sampling_freq=20):\n",
    "    df = pd.DataFrame({'time':time,'x':x,'y':y,'z':z})\n",
    "    df = df.sort_values('time').reset_index(drop=True)\n",
    "    f = interpolate.interp1d(df['time'].values, df[['x','y','z']].values,axis=0)\n",
    "    x_new = np.linspace(df['time'].min(),df['time'].max(),window_size*desired_sampling_freq)\n",
    "    return f(x_new).reshape(-1,3)\n",
    "\n",
    "\n",
    "window_size  = 5\n",
    "desired_sampling_freq = 20\n",
    "\n",
    "schema2 = StructType([\n",
    "    StructField(\"version\", IntegerType()),\n",
    "    StructField(\"user\", StringType()),\n",
    "    StructField(\"timestamp\", TimestampType()),\n",
    "    StructField(\"localtime\", TimestampType()),\n",
    "    StructField(\"start\", TimestampType()),\n",
    "    StructField(\"end\", TimestampType()),\n",
    "    StructField(\"heart_rate\", DoubleType()),\n",
    "    StructField(\"indicator\", DoubleType()),\n",
    "    StructField(\"acl_std\", DoubleType())\n",
    "])\n",
    "@pandas_udf(schema2, PandasUDFType.GROUPED_MAP)\n",
    "def get_heart_rate(df):\n",
    "    df  = df.sort_values('timestamp').reset_index(drop=True)\n",
    "    df['ppg_shape'] = df['ppg1'].apply(lambda a:len(a))\n",
    "    df = df[df['ppg_shape']>.6*100*5]\n",
    "    df['acl_shape'] = df['x'].apply(lambda a:len(a))\n",
    "    df = df[df['acl_shape']>.6*50*5]\n",
    "    df['ppg_window'] = df.apply(lambda a:interpolate_ppg(a['ppg_time'],a['ppg1'],window_size,desired_sampling_freq),axis=1)\n",
    "    df['iqr'] = df['ppg_window'].apply(lambda a:iqr(a))\n",
    "    df = df[df['iqr']>500]\n",
    "    df['acl_std'] = df.apply(lambda a:np.sqrt(np.std(a['x'])**2+np.std(a['y'])**2+np.std(a['z'])**2),axis=1)\n",
    "    df['acl_window'] = df.apply(lambda a:interpolate_acl(a['time'],a['x'],a['y'],a['z'],window_size,desired_sampling_freq),axis=1)\n",
    "    heart_rate_class = heart_rate()\n",
    "    df['heart_rate'] = df.apply(lambda a:heart_rate_class.get_rr_value(a['ppg_window'],a['acl_window'],a['time'][0]),axis=1)\n",
    "    df['indicator'] = df['heart_rate'].apply(lambda a:a[1])\n",
    "    df['heart_rate'] = df['heart_rate'].apply(lambda a:a[0])\n",
    "    df = df[df['heart_rate']>0]\n",
    "    return df[['timestamp','localtime','user','version','acl_std','heart_rate','indicator','start','end']]\n",
    "\n",
    "\n",
    "data = CC.get_stream(\"org.md2k.fossil.ppg.filtered.acl.5secs\")\n",
    "groupbycols = [\"user\",\"version\"] + ['day']\n",
    "heart_rate_data = data._data.groupBy(groupbycols).apply(get_heart_rate) \n",
    "schema = heart_rate_data.schema\n",
    "stream_metadata = Metadata()\n",
    "stream_metadata.set_name(\"org.md2k.fossil.heart.rate\").set_description('heart rate from ppg')\n",
    "for field in schema.fields:\n",
    "    stream_metadata.add_dataDescriptor(\n",
    "        DataDescriptor().set_name(str(field.name)).set_type(str(field.dataType))\n",
    "    )\n",
    "stream_metadata.add_module(\n",
    "    ModuleMetadata().set_name('heart rate from ppg') \\\n",
    "    .set_attribute(\"url\", \"https://md2k.org\").set_author(\n",
    "        \"Md Azim Ullah\", \"mullah@memphis.edu\"))\n",
    "ds = DataStream(data=heart_rate_data,metadata=stream_metadata)\n",
    "CC.save_stream(ds,overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CC.get_stream(\"org.md2k.fossil.heart.rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling Routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_rate_data = CC.get_stream(\"org.md2k.fossil.heart.rate\").drop('timestamp','localtime','version','acl_std')\n",
    "acl_data = CC.get_stream(\"org.md2k.fossil.acl.5secs\")\n",
    "\n",
    "all_data = acl_data.join(heart_rate_data,on=['start','end','user'],how='left')\n",
    "\n",
    "all_data = all_data.withColumn('day',F.date_format('localtime',\"yyyyMMdd\"))\n",
    "groupbycols = [\"user\",\"version\"] + ['day']\n",
    "\n",
    "schema2 = StructType([\n",
    "    StructField(\"version\", IntegerType()),\n",
    "    StructField(\"user\", StringType()),\n",
    "    StructField(\"timestamp\", TimestampType()),\n",
    "    StructField(\"localtime\", TimestampType()),\n",
    "    StructField(\"heart_rate\", DoubleType()),\n",
    "    StructField(\"indicator\", DoubleType()),\n",
    "    StructField(\"acl_std\", DoubleType()),\n",
    "    StructField(\"day\", StringType())\n",
    "])\n",
    "@pandas_udf(schema2, PandasUDFType.GROUPED_MAP)\n",
    "def get_acl_std(df):\n",
    "    df['acl_shape'] = df['x'].apply(lambda a:len(a))\n",
    "    df = df[df['acl_shape']>.2*50*5]\n",
    "    df['acl_std'] = df.apply(lambda a:np.sqrt(np.std(a['x'])**2+np.std(a['y'])**2+np.std(a['z'])**2),axis=1)\n",
    "    return df[['acl_std','timestamp','localtime','version','heart_rate','indicator','user','day']]\n",
    "\n",
    "heart_rate_data = all_data._data.groupBy(groupbycols).apply(get_acl_std)\n",
    "schema = heart_rate_data.schema\n",
    "stream_metadata = Metadata()\n",
    "stream_metadata.set_name(\"org.md2k.fossil.heart.rate.acl.std.5.secs\").set_description('heart rate from ppg')\n",
    "for field in schema.fields:\n",
    "    stream_metadata.add_dataDescriptor(\n",
    "        DataDescriptor().set_name(str(field.name)).set_type(str(field.dataType))\n",
    "    )\n",
    "stream_metadata.add_module(\n",
    "    ModuleMetadata().set_name('heart rate from ppg combined with acl standard deviation in 5 seconds windows') \\\n",
    "    .set_attribute(\"url\", \"https://md2k.org\").set_author(\n",
    "        \"Md Azim Ullah\", \"mullah@memphis.edu\"))\n",
    "ds = DataStream(data=heart_rate_data,metadata=stream_metadata)\n",
    "CC.save_stream(ds,overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worn_data = CC.get_stream('worn--org.md2k.watch--fossil_watch_sport').drop('version','localtime')\n",
    "\n",
    "window_size = 60\n",
    "win = F.window(\"timestamp\", windowDuration=str(window_size)+' seconds',startTime='0 seconds')\n",
    "groupbycols = [\"user\"] + [win]\n",
    "worn_data = worn_data._data.groupBy(groupbycols).agg(F.mean('worn')).withColumnRenamed('avg(worn)','worn')\n",
    "\n",
    "data = CC.get_stream(\"org.md2k.fossil.heart.rate.acl.std.5.secs\")\n",
    "window_size = 60\n",
    "win = F.window(\"timestamp\", windowDuration=str(window_size)+' seconds',startTime='0 seconds')\n",
    "groupbycols = [\"user\",\"version\",'day'] + [win]\n",
    "\n",
    "data = data._data.groupBy(groupbycols).agg(F.collect_list('acl_std'),F.collect_list('heart_rate'),F.collect_list('timestamp'),F.collect_list('localtime'))\n",
    "\n",
    "data = data.withColumnRenamed('collect_list(heart_rate)','heart_rate')\n",
    "data = data.withColumnRenamed('collect_list(timestamp)','timestamp')\n",
    "data = data.withColumnRenamed('collect_list(localtime)','localtime')\n",
    "data = data.withColumnRenamed('collect_list(acl_std)','acl_std')\n",
    "data = data.withColumn('timestamp',F.col('timestamp').getItem(0))\n",
    "data = data.withColumn('localtime',F.col('localtime').getItem(0))\n",
    "\n",
    "data = data.join(worn_data,on=['user','window'],how='left').drop('window')\n",
    "\n",
    "data = data.withColumn('time',F.col('timestamp').cast('double'))\n",
    "\n",
    "# import pickle\n",
    "# pickle.dump(data.toPandas(),open('../data/temp.p','wb'))\n",
    "\n",
    "class Sample:\n",
    "    def __init__(self):\n",
    "        self.acl_std = []\n",
    "        self.cur_time = 0\n",
    "        self.last_time = 0\n",
    "        self.wait_time = 10*60\n",
    "        self.must_sample = 20*60\n",
    "        self.sample_decision = 1\n",
    "        self.good_time = -np.inf\n",
    "        self.count = 0\n",
    "        self.last_sampled = 0\n",
    "        self.threshold = 0.015\n",
    "        self.min_threshold = .01\n",
    "        \n",
    "    def get_decision(self,acl_std,cur_time,hr):\n",
    "        temp = self.sample_decision\n",
    "        self.last_time = self.cur_time\n",
    "        self.cur_time = cur_time\n",
    "        self.acl_std.append(acl_std)\n",
    "        self.acl_std = np.array(self.acl_std[-3:])\n",
    "        if self.sample_decision==1:\n",
    "            if len(hr)>0:\n",
    "                self.count = 0\n",
    "                self.good_time = self.last_time\n",
    "            else:\n",
    "                self.count+=1\n",
    "#         elif len(self.acl_std[self.acl_std>=self.min_threshold])>1:\n",
    "#             self.count -= .5 \n",
    "        elif len(self.acl_std[self.acl_std>=self.threshold])>1:\n",
    "            self.count -= 1 \n",
    "        \n",
    "        \n",
    "                \n",
    "        if  self.cur_time-self.last_sampled>=self.must_sample:\n",
    "            self.count = 0\n",
    "            self.sample_decision = 1\n",
    "        \n",
    "        elif self.cur_time - self.last_time>=self.wait_time:\n",
    "            self.count = 0\n",
    "            self.acl_std = self.acl_std[-1:]\n",
    "            self.sample_decision = 1\n",
    "        \n",
    "        elif self.good_time==self.last_time:\n",
    "            self.sample_decision = 1\n",
    "        \n",
    "        elif len(self.acl_std)<2:\n",
    "            self.sample_decision = 1\n",
    "        \n",
    "        elif self.count>=3:\n",
    "#             self.count-=.5\n",
    "            self.sample_decision = 0\n",
    "        \n",
    "        elif len(self.acl_std[self.acl_std>=self.threshold])>2:\n",
    "            if acl_std>3:\n",
    "                if self.count>3:\n",
    "                    self.sample_decision = 0\n",
    "                else:\n",
    "                    self.sample_decision = 1\n",
    "            else:\n",
    "                self.sample_decision = 1\n",
    "        elif len(self.acl_std[self.acl_std<self.threshold])>2:\n",
    "            if self.count<=3:\n",
    "                self.sample_decision = 1\n",
    "            else:\n",
    "                self.sample_decision = 0\n",
    "    \n",
    "        \n",
    "        else:\n",
    "            self.sample_decision = 0\n",
    "        \n",
    "        if self.sample_decision==1:\n",
    "            self.last_sampled = self.cur_time \n",
    "        self.acl_std = list(self.acl_std)\n",
    "        return self.sample_decision\n",
    "\n",
    "        \n",
    "schema2 = StructType([\n",
    "    StructField(\"version\", IntegerType()),\n",
    "    StructField(\"user\", StringType()),\n",
    "    StructField(\"timestamp\", TimestampType()),\n",
    "    StructField(\"localtime\", TimestampType()),\n",
    "    StructField(\"heart_rate\", DoubleType()),\n",
    "    StructField(\"sample_decision\", DoubleType()),\n",
    "    StructField(\"acl_std\", DoubleType()),\n",
    "    StructField(\"day\", StringType()),\n",
    "    StructField(\"worn\", DoubleType())\n",
    "    \n",
    "])\n",
    "from scipy.stats import mode\n",
    "@pandas_udf(schema2, PandasUDFType.GROUPED_MAP)\n",
    "def get_sampling_decision(df):\n",
    "    df['acl_shape'] = df['acl_std'].apply(lambda a:len(a))\n",
    "    df = df[df['acl_shape']>15]\n",
    "    df['acl_std'] = df['acl_std'].apply(lambda a:np.nanpercentile(a,20))\n",
    "    sampling_decision = [1]\n",
    "    df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "    print(df['day'][0])\n",
    "    print('--'*20)\n",
    "    sample_class = Sample()\n",
    "    for i,row in df.iterrows():\n",
    "        if i==len(df)-1:\n",
    "            continue\n",
    "        if sampling_decision[-1]==1:\n",
    "            hr = [a for a in row['heart_rate'] if a is not None or not np.isnan(a)]\n",
    "        else:\n",
    "            hr = []\n",
    "        sampling_decision.append(sample_class.get_decision(row['acl_std'],row['time'],hr))\n",
    "    df['sample_decision'] = sampling_decision\n",
    "    df['heart_rate'] = df['heart_rate'].apply(lambda a:np.median(a) if len(a)>0 else 0)\n",
    "    return df[['user','day','version','localtime','timestamp','acl_std','heart_rate','sample_decision','worn']]\n",
    "    \n",
    "heart_rate_data = data.groupBy(['user','day','version']).apply(get_sampling_decision)\n",
    "schema = heart_rate_data.schema\n",
    "stream_metadata = Metadata()\n",
    "stream_metadata.set_name(\"org.md2k.fossil.heart.rate.acl.std.sample.decision.60.secs\").set_description('Sampling Routine')\n",
    "for field in schema.fields:\n",
    "    stream_metadata.add_dataDescriptor(\n",
    "        DataDescriptor().set_name(str(field.name)).set_type(str(field.dataType))\n",
    "    )\n",
    "stream_metadata.add_module(\n",
    "    ModuleMetadata().set_name('Sampling Routine with minute level heart rate') \\\n",
    "    .set_attribute(\"url\", \"https://md2k.org\").set_author(\n",
    "        \"Md Azim Ullah\", \"mullah@memphis.edu\"))\n",
    "ds = DataStream(data=heart_rate_data,metadata=stream_metadata)\n",
    "CC.save_stream(ds,overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_rate_data = CC.get_stream(\"org.md2k.fossil.heart.rate.acl.std.sample.decision.60.secs\")\n",
    "\n",
    "heart_rate_data = heart_rate_data.withColumn('time',F.col('timestamp').cast('double'))\n",
    "heart_rate_data = heart_rate_data.withColumn('day',F.date_format('localtime',\"yyyyMMdd\"))\n",
    "\n",
    "log_data = CC.get_stream('moods_logs')\n",
    "# log_data = log_data.withColumn('time', F.col('timestamp').cast('timestamp')-\"5 hours\")\n",
    "log_data = log_data.withColumn('day',F.date_format('timestamp',\"yyyyMMdd\")).toPandas()\n",
    "# log_data = log_data.withColumn('time',F.col('timestamp').cast('double')).toPandas()\n",
    "log_data['ind'] = log_data['message'].apply(lambda a:a.find('ACTIVITY INPUT'))\n",
    "log_data = log_data[log_data.ind>-1]\n",
    "\n",
    "log_data['message'] = log_data['message'].apply(lambda a:a[17:-1])\n",
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "log_data['timestamp'] = log_data['timestamp'].apply(lambda x:parse(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data1 = heart_rate_data.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data1['time'] = temp_data1['timestamp'].apply(lambda a:a.timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_data['time'] = log_data['timestamp'].apply(lambda a:a.timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "count = 0\n",
    "plt.rcParams.update({'font.size':25})\n",
    "import matplotlib.pyplot as plt\n",
    "for user in np.unique(temp_data1['user'].values):\n",
    "    for day in np.unique(temp_data1['day'].values):\n",
    "        temp_data = temp_data1[(temp_data1.day==day)&(temp_data1.user==user)].sort_values('timestamp').reset_index(drop=True)\n",
    "        temp_data.fillna(-1,inplace=True)\n",
    "        temp_log = log_data[(log_data.uuid==user)&(log_data.time>=temp_data.time.min())&(log_data.time<=temp_data.time.max())]\n",
    "        print(temp_log.shape)\n",
    "        if temp_data.shape[0]<10:\n",
    "            continue\n",
    "        temp_data['localtime'] = temp_data['localtime'].apply(lambda a:a.strftime(\"%H:%M\"))\n",
    "        fig,ax = plt.subplots(4,1,figsize=(20,15),sharex=True)\n",
    "        \n",
    "        ax[0].plot(temp_data[temp_data['heart_rate']>0]['timestamp'],temp_data[temp_data['heart_rate']>0]['heart_rate'],'--*')\n",
    "        ax[1].bar(temp_data['timestamp'],temp_data['acl_std'],0.001,color='red')\n",
    "        ax[2].plot(temp_data['timestamp'],temp_data['sample_decision'],'--o',color='green')\n",
    "        ax[0].set_title('Heart Rate')\n",
    "        ax[1].set_title('20th percentile of accelerometer standard deviation')\n",
    "        ax[2].set_title('Sampling Decisions')\n",
    "        ax[3].set_title('Logs and Google Wear')\n",
    "        \n",
    "        \n",
    "#         ax[0].set_title('Sampling Decision')\n",
    "    #     plt.show()\n",
    "    #     plt.figure(figsize=(10,10))\n",
    "#         ax[1].bar(temp_data['timestamp'],temp_data['std'],.001,color='red')\n",
    "#         ax[0].set_title(temp_data['day'][0])\n",
    "        plt.xlabel(temp_data['user'][0]+'---'+temp_data['day'][0])\n",
    "#         ax[2].bar(temp_data['timestamp'],iqr_list,.001,color='green')\n",
    "#         ax[2].set_title('Status of PPG Data')\n",
    "        \n",
    "#         plt.setp(ax[2],xticks= temp_log['timestamp'],xticklabels=temp_log['timestamp'])\n",
    "        for i,row in temp_log.iterrows():\n",
    "            ax[3].text(row['timestamp'], 0.05, row['message'],fontsize=15,rotation=90)\n",
    "        ax[3].vlines(temp_data[temp_data.worn>-1]['timestamp'],0,1)\n",
    "        plt.setp(ax[3],xticks= np.array(temp_data['timestamp'])[np.arange(0,temp_data.shape[0],40)],xticklabels=np.array(temp_data['localtime'])[np.arange(0,temp_data.shape[0],40)])\n",
    "        plt.xticks(rotation=60)\n",
    "        from datetime import timedelta\n",
    "#         ax[3].set_xlim([temp_data['timestamp'].min()-timedelta(hours=1),temp_data['timestamp'].max()+timedelta(hours=1)])\n",
    "        plt.savefig('../images_sampling/'+str(count)+'.pdf',dps=1e6)\n",
    "        count+=1\n",
    "        plt.show()\n",
    "        print('-'*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = heart_rate_data._data.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_rate_data.sort(F.col('timestamp')).show(8000,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_rate_data.show(80000,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data1.user.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/mullah/moods/images_sampling.zip'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutil.make_archive('../images_sampling/','zip','../images_sampling/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skew,kurtosis\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "from scipy import signal\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import StructField, StructType, DoubleType, StringType, TimestampType, IntegerType, ArrayType\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from cerebralcortex.core.datatypes import DataStream\n",
    "from cerebralcortex.core.metadata_manager.stream.metadata import Metadata, DataDescriptor, \\\n",
    "    ModuleMetadata\n",
    "\n",
    "### Peak Detection codes ##\n",
    "\n",
    "def _datacheck_peakdetect(x_axis, y_axis):\n",
    "    \"\"\"\n",
    "    check data for peak detection\n",
    "\n",
    "    :param x_axis: time\n",
    "    :param y_axis: values\n",
    "    :return: same as input data\n",
    "    \"\"\"\n",
    "    if x_axis is None:\n",
    "        x_axis = range(len(y_axis))\n",
    "\n",
    "    if len(y_axis) != len(x_axis):\n",
    "        raise ValueError(\"Input vectors y_axis and x_axis must have same length\")\n",
    "\n",
    "    #needs to be a numpy array\n",
    "    y_axis = np.array(y_axis)\n",
    "    x_axis = np.array(x_axis)\n",
    "    return x_axis, y_axis\n",
    "\n",
    "\n",
    "def peakdetect(y_axis, x_axis = None, lookahead = 200, delta=0):\n",
    "    \"\"\"\n",
    "\n",
    "    :param y_axis: values\n",
    "    :param x_axis: time\n",
    "    :param lookahead: steps ahead to look for\n",
    "    :param delta:\n",
    "    :return: peak locations\n",
    "\n",
    "    \"\"\"\n",
    "    max_peaks = []\n",
    "    min_peaks = []\n",
    "    dump = []   #Used to pop the first hit\n",
    "\n",
    "    # check input data\n",
    "    x_axis, y_axis = _datacheck_peakdetect(x_axis, y_axis)\n",
    "    # store data length for later use\n",
    "    length = len(y_axis)\n",
    "\n",
    "\n",
    "    #perform some checks\n",
    "    if lookahead < 1:\n",
    "        raise ValueError(\"Lookahead must be '1' or above in value\")\n",
    "    if not (np.isscalar(delta) and delta >= 0):\n",
    "        raise ValueError(\"delta must be a positive number\")\n",
    "\n",
    "    #maxima and minima candidates are temporarily stored in\n",
    "    #mx and mn respectively\n",
    "    mn, mx = np.Inf, -np.Inf\n",
    "\n",
    "    #Only detect peak if there is 'lookahead' amount of points after it\n",
    "    for index, (x, y) in enumerate(zip(x_axis[:-lookahead],\n",
    "                                       y_axis[:-lookahead])):\n",
    "        if y > mx:\n",
    "            mx = y\n",
    "            mxpos = x\n",
    "        if y < mn:\n",
    "            mn = y\n",
    "            mnpos = x\n",
    "\n",
    "        ####look for max####\n",
    "        if y < mx-delta and mx != np.Inf:\n",
    "            #Maxima peak candidate found\n",
    "            #look ahead in signal to ensure that this is a peak and not jitter\n",
    "            if y_axis[index:index+lookahead].max() < mx:\n",
    "                max_peaks.append([mxpos, mx])\n",
    "                dump.append(True)\n",
    "                #set algorithm to only find minima now\n",
    "                mx = np.Inf\n",
    "                mn = np.Inf\n",
    "                if index+lookahead >= length:\n",
    "                    #end is within lookahead no more peaks can be found\n",
    "                    break\n",
    "                continue\n",
    "\n",
    "        ####look for min####\n",
    "        if y > mn+delta and mn != -np.Inf:\n",
    "            #Minima peak candidate found\n",
    "            #look ahead in signal to ensure that this is a peak and not jitter\n",
    "            if y_axis[index:index+lookahead].min() > mn:\n",
    "                min_peaks.append([mnpos, mn])\n",
    "                dump.append(False)\n",
    "                #set algorithm to only find maxima now\n",
    "                mn = -np.Inf\n",
    "                mx = -np.Inf\n",
    "                if index+lookahead >= length:\n",
    "                    #end is within lookahead no more peaks can be found\n",
    "                    break\n",
    "\n",
    "    #Remove the false hit on the first value of the y_axis\n",
    "    try:\n",
    "        if dump[0]:\n",
    "            max_peaks.pop(0)\n",
    "        else:\n",
    "            min_peaks.pop(0)\n",
    "        del dump\n",
    "    except IndexError:\n",
    "        pass\n",
    "\n",
    "    return [max_peaks, min_peaks]\n",
    "\n",
    "### CQP quality features and heart rate estimation\n",
    "\n",
    "def get_predict_prob(window):\n",
    "    \"\"\"\n",
    "    Get CQP quality features\n",
    "    :param window: Numpy array of PPG data\n",
    "    :return: quality features\n",
    "    \"\"\"\n",
    "    no_channels = window.shape[1]\n",
    "    window[:,:] = signal.detrend(RobustScaler().fit_transform(window),axis=0)\n",
    "    f,pxx = signal.welch(window,fs=100,nperseg=len(window),nfft=10000,axis=0)\n",
    "    pxx = np.abs(pxx)\n",
    "    pxx = MinMaxScaler().fit_transform(pxx)\n",
    "    skews = skew(window,axis=0).reshape(no_channels,1)\n",
    "    kurs = kurtosis(window,axis=0).reshape(no_channels,1)\n",
    "    iqrs = np.std(window,axis=0).reshape(no_channels,1)\n",
    "    rps = np.divide(np.trapz(pxx[np.where((f>=.8)&(f<=2.5))[0]],axis=0),np.trapz(pxx,axis=0)).reshape(no_channels,1)\n",
    "    features = np.concatenate([skews,kurs,rps,iqrs],axis=1)\n",
    "    return features\n",
    "\n",
    "def get_rr_value(values,fs=100):\n",
    "    \"\"\"\n",
    "    Get Mean RR interval\n",
    "\n",
    "    :param values: single channel ppg data\n",
    "    :param fs: sampling frequency\n",
    "    :return: Mean RR interval Information\n",
    "    \"\"\"\n",
    "    try:\n",
    "        f, pxx = signal.welch(values,fs=fs,nperseg=values.shape[0],nfft=10000,axis=0)\n",
    "        f = f.reshape(-1)\n",
    "        pxx = pxx.reshape(-1,1)\n",
    "        peakind =  peakdetect(pxx[:,0],lookahead=2)\n",
    "        x = []\n",
    "        y = []\n",
    "        for a in peakind[0]:\n",
    "            x.append(a[0])\n",
    "            y.append(a[1])\n",
    "        x = np.array(x)\n",
    "        x = x[f[x]>.8]\n",
    "        x = x[f[x]<2.5]\n",
    "        f = f[x]\n",
    "        pxx = pxx[x,0]\n",
    "        return 60000/(60*f[np.argmax(pxx)])\n",
    "    except Exception as e:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def get_rr_and_features(window):\n",
    "    \"\"\"\n",
    "\n",
    "    :param window:\n",
    "    :return: tuple of mean RR interval and Quality features calculated\n",
    "    \"\"\"\n",
    "    no_channels = window.shape[1]\n",
    "    starts = [0]\n",
    "    ends = [125]\n",
    "    rrs = []\n",
    "    features= []\n",
    "    for i,s in enumerate(starts):\n",
    "        e = ends[i]\n",
    "        for j in range(window.shape[1]):\n",
    "            rrs.append(get_rr_value(window[s:,j]))\n",
    "        features.append(get_predict_prob(window[s:e,:]).reshape(1,no_channels,4))\n",
    "    return np.array(rrs),np.concatenate(features).reshape(no_channels,4)\n",
    "\n",
    "\n",
    "\n",
    "def get_metadata_features_rr(data,\n",
    "                 wrist='left',\n",
    "                 sensor_name='motionsensehrv'):\n",
    "    \"\"\"\n",
    "    :param data: input stream\n",
    "    :param wrist: which wrist the data was collected from\n",
    "    :param sensor_name: name of sensor\n",
    "\n",
    "    :return: metadata of output stream\n",
    "    \"\"\"\n",
    "    stream_name = \"org.md2k.\"+str(sensor_name)+\".\"+str(wrist)+\".wrist.features.activity.std\"\n",
    "    stream_metadata = Metadata()\n",
    "    stream_metadata.set_name(stream_name).set_description(\"PPG data quality features and mean RR interval computed from fixed window\") \\\n",
    "        .add_dataDescriptor(DataDescriptor().set_name(\"timestamp\").set_type(\"datetime\")) \\\n",
    "        .add_dataDescriptor(DataDescriptor().set_name(\"localtime\").set_type(\"datetime\")) \\\n",
    "        .add_dataDescriptor(DataDescriptor().set_name(\"version\").set_type(\"int\")) \\\n",
    "        .add_dataDescriptor(DataDescriptor().set_name(\"user\").set_type(\"string\")) \\\n",
    "        .add_dataDescriptor(DataDescriptor().set_name(\"features\").set_type(\"array\")) \\\n",
    "        .add_dataDescriptor(DataDescriptor().set_name(\"rr\").set_type(\"array\")) \\\n",
    "        .add_dataDescriptor(DataDescriptor().set_name(\"activity\").set_type(\"double\")) \\\n",
    "        .add_dataDescriptor(DataDescriptor().set_name(\"start\").set_type(\"timestamp\")) \\\n",
    "        .add_dataDescriptor(DataDescriptor().set_name(\"end\").set_type(\"timestamp\"))\n",
    "\n",
    "    stream_metadata.add_module(\n",
    "        ModuleMetadata().set_name(\"PPG data quality features and  mean RR Interval computed from PPG\")\n",
    "            .set_attribute(\"url\", \"http://md2k.org/\")\n",
    "            .set_author(\"Md Azim Ullah\", \"mullah@memphis.edu\"))\n",
    "    return stream_metadata\n",
    "\n",
    "\n",
    "def compute_quality_features_and_rr(data,\n",
    "                                    Fs=100,\n",
    "                                    window_size=5.0,\n",
    "                                    acceptable_percentage=0.8,\n",
    "                                    ppg_columns=['red','infrared','green'],\n",
    "                                    acl_columns=['aclx','acly','aclz'],\n",
    "                                    wrist='left',\n",
    "                                    sensor_name='motionsensehrv'):\n",
    "    \"\"\"\n",
    "\n",
    "    :param data: Input data\n",
    "    :param Fs: Sampling Frequency\n",
    "    :param window_size: Window size to compute features from\n",
    "    :param acceptable_percentage: minimum acceptable data fraction\n",
    "    :param ppg_columns: columns in input data belonging to PPG\n",
    "    :param acl_columns: columns in input data belonging to Accelerometer\n",
    "    :param wrist: wrist on which the sensor was worn\n",
    "    :param sensor_name: name of sensor\n",
    "    :return: Dataframe containing PPG data quality features and mean RR interval information\n",
    "    \"\"\"\n",
    "\n",
    "    ## check if all columns exist\n",
    "\n",
    "    default_columns = ['user','version','localtime','timestamp']\n",
    "    required_columns = default_columns+list(acl_columns)+list(ppg_columns)\n",
    "    if len(set(required_columns)-set(data.columns))>0:\n",
    "        raise Exception(\"Columns missing in input dataframe! \" + str(list(set(required_columns)-set(data.columns))))\n",
    "\n",
    "    ## select the columns from input dataframe\n",
    "\n",
    "    data = data.select(*required_columns)\n",
    "\n",
    "    ## udf\n",
    "    default_schema = [StructField(\"timestamp\", TimestampType()),\n",
    "                      StructField(\"localtime\", TimestampType()),\n",
    "                      StructField(\"version\", IntegerType()),\n",
    "                      StructField(\"user\", StringType())]\n",
    "    output_schema = [StructField(\"features\", ArrayType(DoubleType())),\n",
    "                     StructField(\"rr\", ArrayType(DoubleType())),\n",
    "                     StructField(\"activity\", DoubleType()),\n",
    "                     StructField(\"start\", TimestampType()),\n",
    "                     StructField(\"end\", TimestampType())]\n",
    "    schema = StructType(default_schema+output_schema)\n",
    "    @pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "    def ppg_features_compute(key,data):\n",
    "        if data.shape[0]>window_size*Fs*acceptable_percentage:\n",
    "            data = data.sort_values('timestamp').reset_index(drop=True)\n",
    "            rows = []\n",
    "            rows.append(data['user'].loc[0])\n",
    "            rows.append(data['version'].loc[0])\n",
    "            rows.append(data['timestamp'].loc[0])\n",
    "            rows.append(data['localtime'].loc[0])\n",
    "            rrs , features = get_rr_and_features(data[list(ppg_columns)].values.reshape(-1,len(ppg_columns)))\n",
    "            rows.append(rrs)\n",
    "            rows.append(features.reshape(-1))\n",
    "            data_acl = data[list(acl_columns)]\n",
    "            values_acl = data_acl.values\n",
    "            acl_std = np.std(values_acl,axis=0)\n",
    "            acl_std = np.sqrt(np.sum(np.square(acl_std)))\n",
    "            rows.append(acl_std)\n",
    "            rows.append(key[2]['start'])\n",
    "            rows.append(key[2]['end'])\n",
    "            return pd.DataFrame([rows],columns=['user','version',\n",
    "                                                'timestamp','localtime',\n",
    "                                                'rr','features','activity',\n",
    "                                                'start','end'])\n",
    "\n",
    "        else:\n",
    "            return pd.DataFrame([],columns=['user','version',\n",
    "                                            'timestamp','localtime',\n",
    "                                            'rr','features','activity',\n",
    "                                            'start','end'])\n",
    "\n",
    "    ppg_features_and_rr = data.compute(ppg_features_compute,windowDuration=window_size,slideDuration=2,startTime='0 seconds')\n",
    "    output_data = ppg_features_and_rr._data\n",
    "    ds = DataStream(data=output_data,metadata=get_metadata_features_rr(data,wrist=wrist,sensor_name=sensor_name))\n",
    "\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CC.get_stream('org.md2k.fossil.left.wrist.bandpass.filtered')\n",
    "\n",
    "data = data.withColumn('aclx',F.lit(1))\n",
    "data = data.withColumn('acly',F.lit(2))\n",
    "data = data.withColumn('aclz',F.lit(3))\n",
    "\n",
    "rr_data = compute_quality_features_and_rr(data,\n",
    "                                    Fs=100,\n",
    "                                    window_size=5.0,\n",
    "                                    acceptable_percentage=0.8,\n",
    "                                    ppg_columns=['ppg1'],\n",
    "                                    acl_columns=['aclx','acly','aclz'],\n",
    "                                    wrist='left',\n",
    "                                    sensor_name='fossil')\n",
    "final_data = rr_data.drop('activity')._data.withColumn('rr',F.col('rr').getItem(0))\n",
    "schema = final_data.schema\n",
    "stream_metadata = Metadata()\n",
    "stream_metadata.set_name(\"org.md2k.fossil.left.wrist.features.rr\").set_description(\"Bandpass Filtered PPG, ECG Rpeak\")\n",
    "for field in schema.fields:\n",
    "    stream_metadata.add_dataDescriptor(\n",
    "        DataDescriptor().set_name(str(field.name)).set_type(str(field.dataType))\n",
    "    )\n",
    "stream_metadata.add_module(\n",
    "    ModuleMetadata().set_name(\"Bandpass Filtered PPG, ECG Rpeak\") \\\n",
    "    .set_attribute(\"url\", \"https://md2k.org\").set_author(\n",
    "        \"Md Azim Ullah\", \"mullah@memphis.edu\"))\n",
    "stream_metadata.is_valid()\n",
    "ds = DataStream(data=final_data,metadata=stream_metadata)\n",
    "CC.save_stream(ds,overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data = data.filter(F.col('user')=='d20e1bc7-de8d-38e4-9d97-09e64b88816d').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(temp_data,open('../data/temp_data_mine.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sample:\n",
    "    def __init__(self):\n",
    "        self.acl_std = []\n",
    "        self.cur_time = 0\n",
    "        self.last_time = 0\n",
    "        self.wait_time = 10*60\n",
    "        self.sample_decision = 1\n",
    "        self.good_time = np.inf\n",
    "        \n",
    "        \n",
    "    def get_decision(self,acl_std,cur_time,hr):\n",
    "        self.last_time = self.cur_time\n",
    "        self.cur_time = cur_time\n",
    "        if len(hr)>0:\n",
    "            self.good_time = self.last_time\n",
    "        self.acl_std.append(acl_std)\n",
    "        self.acl_std = np.array(self.acl_std[-3:])\n",
    "        \n",
    "        if self.cur_time - self.last_time>=self.wait_time:\n",
    "            self.acl_std = self.acl_std[-1:]\n",
    "            self.sample_decision = 1\n",
    "        elif len(self.acl_std)<2:\n",
    "            self.sample_decision = 1\n",
    "        elif self.cur_time-self.last_sampled>self.wait_time:\n",
    "            if len(self.acl_std[self.acl_std<.02])>=2:\n",
    "                self.sample_decision = 0\n",
    "            elif self.last_sampled-self.cur_time<self.wait_time/2 and self.good_time-self.cur_time>self.wait_time/2:\n",
    "                self.sample_decision = 0\n",
    "            else:\n",
    "                self.sample_decision = 1\n",
    "        \n",
    "        elif len(self.acl_std[self.acl_std>.02])>1:\n",
    "            if acl_std>5:\n",
    "                if self.cur_time - self.good_time<self.wait_time/2:\n",
    "                    self.sample_decision=1\n",
    "                else:\n",
    "                    self.sample_decision=0\n",
    "            else:\n",
    "                self.sample_decision=1\n",
    "        else:\n",
    "            self.sample_decision=0\n",
    "\n",
    "        if self.sample_decision==1:\n",
    "            self.last_sampled = self.cur_time \n",
    "        self.acl_std = list(self.acl_std)\n",
    "        print(self.cur_time-self.last_time,acl_std,self.cur_time-self.good_time,self.cur_time-self.last_sampled,'----',len(hr),'---',self.sample_decision)\n",
    "        return self.sample_decision\n",
    "def get_sampling_decision(df):\n",
    "    \n",
    "    df['acl_shape'] = df['acl_std'].apply(lambda a:len(a))\n",
    "    df = df[df['acl_shape']>5]\n",
    "    df['acl_std'] = df['acl_std'].apply(lambda a:np.nanpercentile(a,80))\n",
    "    sampling_decision = [1]\n",
    "    df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "    print(df['day'][0])\n",
    "    print('--'*20)\n",
    "    sample_class = Sample()\n",
    "    for i,row in df.iterrows():\n",
    "        if i==len(df)-1:\n",
    "            continue\n",
    "        if sampling_decision[-1]==1:\n",
    "            hr = [a for a in df['heart_rate'].loc[i+1] if a is not None or not np.isnan(a)]\n",
    "        else:\n",
    "            hr = []\n",
    "        sampling_decision.append(sample_class.get_decision(row['acl_std'],row['time'],hr))\n",
    "    df['sample_decision'] = sampling_decision\n",
    "    df['heart_rate'] = df['heart_rate'].apply(lambda a:np.mean(a) if len(a)>0 else 0)\n",
    "    return df[['user','day','version','localtime','timestamp','acl_std','heart_rate','sample_decision']]\n",
    "    \n",
    "heart_rate_data = temp_data.groupby(['user','day','version'],as_index=False).apply(get_sampling_decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CC.get_stream('accelerometer--org.md2k.watch--fossil_watch_sport' ,user_id='afcfc1b5-365f-409b-918e-2f0ce8056ff9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sort(F.col('localtime').desc()).show(1000,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win = F.window(\"timestamp\", windowDuration='5 seconds',slideDuration='2 seconds',startTime='0 seconds')\n",
    "groupbycols = [\"user\",\"version\"] + [win]\n",
    "\n",
    "import numpy as np\n",
    "schema2 = StructType([\n",
    "    StructField(\"version\", IntegerType()),\n",
    "    StructField(\"user\", StringType()),\n",
    "    StructField(\"start\", TimestampType()),\n",
    "    StructField(\"end\", TimestampType()),\n",
    "    StructField(\"activity\", DoubleType())\n",
    "])\n",
    "@pandas_udf(schema2, PandasUDFType.GROUPED_MAP)\n",
    "def compute_std(key,data1):\n",
    "    activity = np.sqrt(data1['x'].std()**2 + data1['y'].std()**2 + data1['z'].std()**2)\n",
    "    temp = [data1.version.values[0],\n",
    "           data1.user.values[0],\n",
    "           key[2]['start'],\n",
    "           key[2]['end'],\n",
    "           activity]\n",
    "    return pd.DataFrame([temp],columns=['version','user','start','end','activity'])\n",
    "activity_data = data._data.groupBy(groupbycols).apply(compute_std)\n",
    "\n",
    "\n",
    "rr_data = CC.get_stream(\"org.md2k.fossil.left.wrist.features.rr\")._data\n",
    "\n",
    "final_data = activity_data.join(rr_data.drop('version'),how='inner',on=['user','start','end'])\n",
    "\n",
    "schema = final_data.schema\n",
    "stream_metadata = Metadata()\n",
    "stream_metadata.set_name(\"org.md2k.fossil.left.wrist.features.rr.activity.std\").set_description(\"Bandpass Filtered PPG, ECG Rpeak\")\n",
    "for field in schema.fields:\n",
    "    stream_metadata.add_dataDescriptor(\n",
    "        DataDescriptor().set_name(str(field.name)).set_type(str(field.dataType))\n",
    "    )\n",
    "stream_metadata.add_module(\n",
    "    ModuleMetadata().set_name(\"Bandpass Filtered PPG, ECG Rpeak\") \\\n",
    "    .set_attribute(\"url\", \"https://md2k.org\").set_author(\n",
    "        \"Md Azim Ullah\", \"mullah@memphis.edu\"))\n",
    "stream_metadata.is_valid()\n",
    "ds = DataStream(data=final_data,metadata=stream_metadata)\n",
    "CC.save_stream(ds,overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "left join to maximize the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CC.get_stream('accelerometer--org.md2k.watch--fossil_watch_sport',user_id='afcfc1b5-365f-409b-918e-2f0ce8056ff9')\n",
    "\n",
    "win = F.window(\"timestamp\", windowDuration='5 seconds',slideDuration='2 seconds',startTime='0 seconds')\n",
    "groupbycols = [\"user\",\"version\"] + [win]\n",
    "\n",
    "import numpy as np\n",
    "schema2 = StructType([\n",
    "    StructField(\"version\", IntegerType()),\n",
    "    StructField(\"user\", StringType()),\n",
    "    StructField(\"start\", TimestampType()),\n",
    "    StructField(\"end\", TimestampType()),\n",
    "    StructField(\"activity\", DoubleType())\n",
    "])\n",
    "@pandas_udf(schema2, PandasUDFType.GROUPED_MAP)\n",
    "def compute_std(key,data1):\n",
    "    activity = np.sqrt(data1['x'].std()**2 + data1['y'].std()**2 + data1['z'].std()**2)\n",
    "    temp = [data1.version.values[0],\n",
    "           data1.user.values[0],\n",
    "           key[2]['start'],\n",
    "           key[2]['end'],\n",
    "           activity]\n",
    "    return pd.DataFrame([temp],columns=['version','user','start','end','activity'])\n",
    "activity_data = data._data.groupBy(groupbycols).apply(compute_std)\n",
    "\n",
    "\n",
    "rr_data = CC.get_stream(\"org.md2k.fossil.left.wrist.features.rr.quality.likelihood\")._data ## see below\n",
    "\n",
    "final_data = activity_data.join(rr_data.drop('version'),how='left',on=['user','start','end'])\n",
    "\n",
    "schema = final_data.schema\n",
    "stream_metadata = Metadata()\n",
    "stream_metadata.set_name(\"org.md2k.fossil.left.wrist.features.rr.activity.std.quality.likelihood.with.null\").set_description(\"Bandpass Filtered PPG, ECG Rpeak\")\n",
    "for field in schema.fields:\n",
    "    stream_metadata.add_dataDescriptor(\n",
    "        DataDescriptor().set_name(str(field.name)).set_type(str(field.dataType))\n",
    "    )\n",
    "stream_metadata.add_module(\n",
    "    ModuleMetadata().set_name(\"Bandpass Filtered PPG, ECG Rpeak\") \\\n",
    "    .set_attribute(\"url\", \"https://md2k.org\").set_author(\n",
    "        \"Md Azim Ullah\", \"mullah@memphis.edu\"))\n",
    "stream_metadata.is_valid()\n",
    "ds = DataStream(data=final_data,metadata=stream_metadata)\n",
    "CC.save_stream(ds,overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "match with ecg rr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_rr = CC.get_stream(\"org.md2k.autosense.ecg.rr.final.hamiltonian.5secs.average\").withColumnRenamed('rr','ecg_rr')\n",
    "\n",
    "ppg_rr = CC.get_stream(\"org.md2k.fossil.left.wrist.features.rr.activity.std\")\n",
    "\n",
    "rr = ppg_rr.join(ecg_rr.drop('version','timestamp','localtime'),how='inner',on=['user','start','end'])\n",
    "\n",
    "data = rr._data.toPandas()\n",
    "\n",
    "import pickle\n",
    "pickle.dump(data,open('../data/ecg_ppg_rr_matched_5secs_motion.p','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot ecg ppg rr based on motion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "data = pickle.load(open('../data/ecg_ppg_rr_matched_5secs_motion.p','rb'))\n",
    "data = data[(data.rr>400)&(data.rr<1200)]\n",
    "data['difference'] = np.abs(data['rr']-data['ecg_rr'])\n",
    "\n",
    "levels = np.linspace(data['activity'].min(),.5,60)\n",
    "data['level'] = data['activity'].apply(lambda a:min(levels, key=lambda x:abs(x-a)))\n",
    "data['level'] = data['level'].apply(lambda a:'{:.4f}'.format(a))\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size':20})\n",
    "plt.figure(figsize=(25,15))\n",
    "sns.boxplot(x='level',y='difference',data=data,showfliers = False)\n",
    "plt.xlabel('Magnitude of standard deviation across 3 acl channels')\n",
    "plt.ylabel('Absolute difference in milliseconds (5 second level mean rr int.)')\n",
    "plt.ylim([0,400])\n",
    "plt.xticks(rotation=60)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compute signal quality likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def get_quality_likelihood(data,\n",
    "                           clf,\n",
    "                           no_of_ppg_channels = 3,\n",
    "                           no_of_quality_features = 4):\n",
    "    ## helper method\n",
    "    def convert_to_array(vals):\n",
    "        return np.array(vals).reshape(no_of_ppg_channels,no_of_quality_features)\n",
    "\n",
    "    ## udf\n",
    "    schema = StructType([\n",
    "        StructField(\"version\", IntegerType()),\n",
    "        StructField(\"user\", StringType()),\n",
    "        StructField(\"localtime\", TimestampType()),\n",
    "        StructField(\"timestamp\", TimestampType()),\n",
    "        StructField(\"likelihood_max\", DoubleType()),\n",
    "        StructField(\"rr\", DoubleType()),\n",
    "        StructField(\"likelihood_max_array\", ArrayType(DoubleType())),\n",
    "        StructField(\"rr_array\", ArrayType(DoubleType())),\n",
    "        StructField(\"activity\", DoubleType()),\n",
    "        StructField(\"start\", TimestampType()),\n",
    "        StructField(\"end\", TimestampType()),\n",
    "        StructField(\"features\", ArrayType(DoubleType())),\n",
    "    ])\n",
    "    @pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "    def ppg_likelihood_compute(data):\n",
    "        if data.shape[0]>0:\n",
    "            data['features'] = data['features'].apply(convert_to_array)\n",
    "            acl_features = np.concatenate(list(data['features'])).reshape(-1, no_of_ppg_channels,no_of_quality_features)\n",
    "            likelihood = []\n",
    "            for k in range(acl_features.shape[1]):\n",
    "                tmp = np.nan_to_num(acl_features[:,k,:]).reshape(-1,no_of_quality_features)\n",
    "                likelihood.append(clf.predict_proba(tmp)[:,1].reshape(-1,1))\n",
    "\n",
    "            likelihood = np.concatenate(likelihood,axis=1)\n",
    "            rrs = data['rr'].values\n",
    "            rrs = np.array([np.array(a) for a in rrs])\n",
    "            likelihood_max = []\n",
    "            rr = []\n",
    "            rr_array = []\n",
    "            likelihood_max_array = []\n",
    "            for i in range(likelihood.shape[0]):\n",
    "                a = likelihood[i,:]\n",
    "                likelihood_max_array.append(list(a))\n",
    "                rr_array.append(list(rrs[i]))\n",
    "                likelihood_max.append(np.max(a))\n",
    "                rr.append(rrs[i][np.argmax(a)])\n",
    "            data['likelihood_max'] = likelihood_max\n",
    "            data['rr'] = rr\n",
    "            data['likelihood_max_array'] = likelihood_max_array\n",
    "            data['rr_array'] = rr_array\n",
    "            data['features'] = data['features'].apply(lambda a:a.reshape(-1))\n",
    "            return data\n",
    "        else:\n",
    "            return pd.DataFrame([],columns=['user','version','timestamp','localtime','likelihood_max',\n",
    "                                            'rr','activity','likelihood_max_array','rr_array','start','end','features'])\n",
    "\n",
    "    ppg_likelihood = data._data.groupBy(['user','version']).apply(ppg_likelihood_compute)\n",
    "    return ppg_likelihood\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "without activity merging first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_clf  = pickle.load(open('/home/jupyter/mullah/Test/data_yield/classifier_sqi/classifier.p','rb'))\n",
    "\n",
    "data = CC.get_stream(\"org.md2k.fossil.left.wrist.features.rr\")\n",
    "\n",
    "data = data.withColumn('rr',F.array('rr'))\n",
    "\n",
    "data = data.withColumn('activity',F.lit(1))\n",
    "\n",
    "final_data = get_quality_likelihood(data,\n",
    "                           quality_clf,\n",
    "                           no_of_ppg_channels = 1,\n",
    "                           no_of_quality_features = 4).drop('activity')\n",
    "\n",
    "schema = final_data.schema\n",
    "stream_metadata = Metadata()\n",
    "stream_metadata.set_name(\"org.md2k.fossil.left.wrist.features.rr.quality.likelihood\").set_description(\"Bandpass Filtered PPG, ECG Rpeak\")\n",
    "for field in schema.fields:\n",
    "    stream_metadata.add_dataDescriptor(\n",
    "        DataDescriptor().set_name(str(field.name)).set_type(str(field.dataType))\n",
    "    )\n",
    "stream_metadata.add_module(\n",
    "    ModuleMetadata().set_name(\"Bandpass Filtered PPG, ECG Rpeak\") \\\n",
    "    .set_attribute(\"url\", \"https://md2k.org\").set_author(\n",
    "        \"Md Azim Ullah\", \"mullah@memphis.edu\"))\n",
    "stream_metadata.is_valid()\n",
    "ds = DataStream(data=final_data,metadata=stream_metadata)\n",
    "CC.save_stream(ds,overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with activity merging done already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_clf  = pickle.load(open('/home/jupyter/mullah/Test/data_yield/classifier_sqi/classifier.p','rb'))\n",
    "\n",
    "data = CC.get_stream(\"org.md2k.fossil.left.wrist.features.rr.activity.std\")\n",
    "\n",
    "data = data.withColumn('rr',F.array('rr'))\n",
    "\n",
    "final_data = get_quality_likelihood(data,\n",
    "                           quality_clf,\n",
    "                           no_of_ppg_channels = 1,\n",
    "                           no_of_quality_features = 4)\n",
    "\n",
    "schema = final_data.schema\n",
    "stream_metadata = Metadata()\n",
    "stream_metadata.set_name(\"org.md2k.fossil.left.wrist.features.rr.activity.std.quality.likelihood\").set_description(\"Bandpass Filtered PPG, ECG Rpeak\")\n",
    "for field in schema.fields:\n",
    "    stream_metadata.add_dataDescriptor(\n",
    "        DataDescriptor().set_name(str(field.name)).set_type(str(field.dataType))\n",
    "    )\n",
    "stream_metadata.add_module(\n",
    "    ModuleMetadata().set_name(\"Bandpass Filtered PPG, ECG Rpeak\") \\\n",
    "    .set_attribute(\"url\", \"https://md2k.org\").set_author(\n",
    "        \"Md Azim Ullah\", \"mullah@memphis.edu\"))\n",
    "stream_metadata.is_valid()\n",
    "ds = DataStream(data=final_data,metadata=stream_metadata)\n",
    "CC.save_stream(ds,overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "match with ecg rr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_rr = CC.get_stream(\"org.md2k.autosense.ecg.rr.final.hamiltonian.5secs.average\").withColumnRenamed('rr','ecg_rr')\n",
    "\n",
    "ppg_rr = CC.get_stream(\"org.md2k.fossil.left.wrist.features.rr.activity.std.quality.likelihood\")\n",
    "\n",
    "rr = ppg_rr.join(ecg_rr.drop('version','timestamp','localtime'),how='inner',on=['user','start','end'])\n",
    "\n",
    "data = rr._data.toPandas()\n",
    "\n",
    "import pickle\n",
    "pickle.dump(data,open('../data/ecg_ppg_rr_matched_5secs.p','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot ecg ppg rr difference based on quality likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "data = pickle.load(open('../data/ecg_ppg_rr_matched_5secs.p','rb'))\n",
    "data = data[(data.rr>300)&(data.rr<1200)&(data.activity<1)]\n",
    "data = data[(data.ecg_rr>300)&(data.ecg_rr<1200)]\n",
    "\n",
    "data['difference'] = np.abs(data['rr']-data['ecg_rr'])\n",
    "data['power'] = data['features'].apply(lambda a:a[2])\n",
    "levels = np.arange(0,1,.1)\n",
    "data['level'] = data['likelihood_max'].apply(lambda a:min(levels, key=lambda x:abs(x-a)))\n",
    "# data['level'] = data['level'].apply(lambda a:'{:.4f}'.format(a))\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size':20})\n",
    "plt.figure(figsize=(25,15))\n",
    "sns.boxplot(x='level',y='difference',data=data,showfliers = False)\n",
    "plt.xlabel('Likelihood')\n",
    "plt.ylabel('Absolute difference in milliseconds')\n",
    "# plt.ylim([0,400])\n",
    "plt.xticks(rotation=60)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "60 seconds PPG RR computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CC.get_stream(\"org.md2k.fossil.left.wrist.features.rr.activity.std.quality.likelihood\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.filter(F.col('rr')>300)\n",
    "data = data.filter(F.col('rr')<1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CC.get_stream(\"org.md2k.fossil.left.wrist.features.rr.activity.std.quality.likelihood.with.null\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win = F.window(\"timestamp\", windowDuration='60 seconds',slideDuration='60 seconds',startTime='0 seconds')\n",
    "groupbycols = [\"user\",\"version\"] + [win]\n",
    "schema2 = StructType([\n",
    "    StructField(\"version\", IntegerType()),\n",
    "    StructField(\"user\", StringType()),\n",
    "    StructField(\"start\", TimestampType()),\n",
    "    StructField(\"end\", TimestampType()),\n",
    "    StructField(\"length\", DoubleType()),\n",
    "    StructField(\"rr_array\", ArrayType(DoubleType())),\n",
    "    StructField(\"timestamp\", TimestampType()),\n",
    "    StructField(\"localtime\", TimestampType()),\n",
    "    StructField(\"activity\", ArrayType(DoubleType())),\n",
    "    StructField(\"likelihood\", ArrayType(DoubleType()))    \n",
    "])\n",
    "@pandas_udf(schema2, PandasUDFType.GROUPED_MAP)\n",
    "def compute_60_secs(key,df):\n",
    "#     if df.shape[0]<.33*30:\n",
    "#         return pd.DataFrame([],columns=['version','user','start','end',\n",
    "#                                         'rr','rr_array','timestamp','localtime',\n",
    "#                                        'activity','likelihood'])\n",
    "    temp = [df.version.values[0],\n",
    "            df.user.values[0],\n",
    "            key[2]['start'],\n",
    "            key[2]['end'],\n",
    "            df.shape[0],\n",
    "            np.array(list(df['rr'])),\n",
    "            df.timestamp.values[0],\n",
    "            df.localtime.values[0],\n",
    "            np.array(list(df['activity'])),\n",
    "            np.array(list(df['likelihood_max']))]\n",
    "    return pd.DataFrame([temp],columns=['version','user','start','end',\n",
    "                                        'length','rr_array','timestamp','localtime',\n",
    "                                       'activity','likelihood'])\n",
    "final_data = data._data.groupby(groupbycols).apply(compute_60_secs)\n",
    "schema = final_data.schema\n",
    "stream_metadata = Metadata()\n",
    "stream_metadata.set_name(\"org.md2k.fossil.left.wrist.rr.activity.likelihood.60.secs\").set_description(\"Bandpass Filtered PPG, ECG Rpeak\")\n",
    "for field in schema.fields:\n",
    "    stream_metadata.add_dataDescriptor(\n",
    "        DataDescriptor().set_name(str(field.name)).set_type(str(field.dataType))\n",
    "    )\n",
    "stream_metadata.add_module(\n",
    "    ModuleMetadata().set_name(\"Bandpass Filtered PPG, ECG Rpeak\") \\\n",
    "    .set_attribute(\"url\", \"https://md2k.org\").set_author(\n",
    "        \"Md Azim Ullah\", \"mullah@memphis.edu\"))\n",
    "stream_metadata.is_valid()\n",
    "ds = DataStream(data=final_data,metadata=stream_metadata)\n",
    "CC.save_stream(ds,overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CC.get_stream(\"org.md2k.fossil.left.wrist.rr.activity.likelihood.60.secs\")\n",
    "data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Match with minute level RR from ECG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CC.get_stream(\"org.md2k.fossil.left.wrist.rr.activity.likelihood.60.secs\")\n",
    "\n",
    "ecg_data = CC.get_stream(\"org.md2k.autosense.ecg.rr.final.hamiltonian.60secs.average\")\n",
    "\n",
    "all_data = data.join(ecg_data.drop('version','timestamp','localtime'),how='left',on=['start','end','user'])\n",
    "\n",
    "data = all_data._data.toPandas()\n",
    "\n",
    "pickle.dump(data,open('../data/60_seconds_ecg_ppg.p','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "experiment with 60 seconds data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open('../data/60_seconds_ecg_ppg.p','rb'))\n",
    "\n",
    "data['activity_std'] = data['activity'].apply(lambda a:np.nanpercentile([b for b in a if b!=None],75))\n",
    "data['ppg_rr'] = data['rr_array'].apply(lambda a:np.nanmean(a))\n",
    "data['ppg_rr'] = data['rr_array'].apply(lambda a:np.nanmean([b for b in a if b>300 and b<1500 and b!=None]))\n",
    "data['mean_likelihood'] = data['likelihood'].apply(lambda a:np.nanmean(a))\n",
    "\n",
    "df = data[['ecg_rr','ppg_rr','activity_std','mean_likelihood','length']].dropna()\n",
    "df['difference'] = np.abs(df['ecg_rr']-df['ppg_rr'])\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.scatter(df['ecg_rr'],df['ppg_rr'],c=df['activity_std'])\n",
    "plt.xlabel('ecg mean rr (60 seconds)')\n",
    "plt.ylabel('PPG mean rr (60 seconds)')\n",
    "# sns.lineplot(x='mean_likelihood',y='difference',data=df)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "levels = np.linspace(0,1,100)\n",
    "df['level'] = df['activity_std'].apply(lambda a:min(levels, key=lambda x:abs(x-a)))\n",
    "df['level'] = df['level'].apply(lambda a:'{:.4f}'.format(a))\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size':20})\n",
    "plt.figure(figsize=(25,15))\n",
    "sns.boxplot(x='level',y='difference',data=df,showfliers = False)\n",
    "plt.xlabel('75th percentile of activity standard deviation')\n",
    "plt.ylabel('Absolute difference in milliseconds')\n",
    "# plt.ylim([0,400])\n",
    "plt.xticks(rotation=60)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "df = df[df['length']>=10]\n",
    "# df = df[df['activity_std']>.0204]\n",
    "df['difference'] = np.abs(df['ecg_rr']-df['ppg_rr'])\n",
    "levels = np.linspace(0,1,10)\n",
    "df['level'] = df['mean_likelihood'].apply(lambda a:min(levels, key=lambda x:abs(x-a)))\n",
    "# data['level'] = data['level'].apply(lambda a:'{:.4f}'.format(a))\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size':20})\n",
    "plt.figure(figsize=(25,15))\n",
    "sns.boxplot(x='level',y='difference',data=df,showfliers = False)\n",
    "plt.xlabel('Mean Likelihood in a minute')\n",
    "plt.ylabel('Absolute difference in milliseconds')\n",
    "# plt.ylim([0,400])\n",
    "plt.xticks(rotation=60)\n",
    "plt.show()\n",
    "\n",
    "df['activity_std'].min(),df['activity_std'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "data.activity_std.hist(bins=400)\n",
    "plt.xlim([0.05,10])\n",
    "plt.ylim([0,50])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppg = pickle.load(open('../data/ppg.p','rb'))\n",
    "acl = pickle.load(open('../data/acl.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "def preProcessing(X1,Fs=100,fil_type='ppg'):\n",
    "    X1 = X1.reshape(-1,1)\n",
    "    X1 = signal.detrend(X1,axis=0,type='constant')\n",
    "    b = signal.firls(65,np.array([0,0.2, 0.3, 3 ,3.5,Fs/2]),np.array([0, 0 ,1 ,1 ,0, 0]),\n",
    "                     np.array([100*0.02,0.02,0.02]),fs=Fs)\n",
    "    X2 = signal.convolve(X1.reshape(-1),b,mode='same')\n",
    "    return X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppg = ppg.sort_values('timestamp').reset_index(drop=True)\n",
    "ppg['filtered_ppg'] = preProcessing(ppg['ppg1'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(ppg['timestamp'][10000:30000],ppg['filtered_ppg'][10000:30000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppg_col = [df for i,df in ppg.groupby(pd.Grouper(key='timestamp',freq='5S'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in ppg_col:\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.plot(df['timestamp'],df['filtered_ppg'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "High Performance CC3.3",
   "language": "python",
   "name": "cc33_high_performance"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
